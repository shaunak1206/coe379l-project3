{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659f2a7a",
   "metadata": {
    "id": "659f2a7a"
   },
   "source": [
    "# Project 3: Fine-Tuning FLAN-T5 for Summarization & Measuring Forgetting\n",
    "\n",
    "**Authors:** Shaunak Kapur & Pranav Krishnan\n",
    "\n",
    "This notebook implements the Project 3 proposal: fine-tuning a small language model (`google/flan-t5-small`) on the Amazon Fine Food Reviews dataset to generate product review summaries. It also evaluates \"forgetting\" by checking the model's performance on a set of general knowledge questions before and after fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50a91d",
   "metadata": {
    "id": "7b50a91d"
   },
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Installing required libraries: `transformers`, `datasets`, `evaluate`, `rouge_score`, `accelerate`, `sentencepiece`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d22324",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44d22324",
    "outputId": "d68e0ee9-c22c-4ab0-873f-06031408168d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets evaluate rouge_score accelerate sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8fb80",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfd8fb80",
    "outputId": "429effb8-794b-46ea-99d6-0d59bfc282d4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1e698",
   "metadata": {
    "id": "d3a1e698"
   },
   "source": [
    "## 2. Load and Preprocess Data\n",
    "\n",
    "We use the Amazon Fine Food Reviews dataset from Hugging Face. The dataset will be automatically downloaded using `load_dataset`.\n",
    "\n",
    "We will:\n",
    "1. Download the dataset from Hugging Face.\n",
    "2. Convert to pandas DataFrame.\n",
    "3. Drop rows with missing values.\n",
    "4. Sample the data (e.g., 20,000 rows) to keep training time reasonable.\n",
    "5. Split into Train (80%), Validation (10%), and Test (10%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db864825",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 687
    },
    "id": "db864825",
    "outputId": "8767f9df-5907-4009-8cc5-be51ab6c77f1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "STEP 1: Downloading dataset from Hugging Face...\n",
      "================================================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d19de1b81524dff9d89d083027ce49f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Reviews.csv:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40514754e60a4965b14357d1ca020f29"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/568454 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5fb0c5bc66a6465eadae6ba3197599e1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Dataset loaded. Available splits: ['train']\n",
      "\n",
      "Converting to pandas DataFrame...\n",
      "\u2713 Original dataset size: 568454 rows, 10 columns\n",
      "  Columns: ['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text']\n",
      "\n",
      "Filtering data...\n",
      "  Before filtering: 568454 rows\n",
      "  After dropping NaN: 568427 rows\n",
      "  After filtering long reviews (<=512 chars): 420179 rows\n",
      "\n",
      "Sampling 20000 rows from 420179 total rows...\n",
      "\u2713 Sampled dataset size: 20000 rows\n",
      "\n",
      "Sample data preview:\n",
      "                 Summary                                               Text\n",
      "310404   Very tasty bars  There are many varieties of bars on the market...\n",
      "74819   Keep on Munching  My puppy loves this product.  The moment he ha...\n",
      "271575       cody cramer  These meatballs look so delicious I wanted to ...\n",
      "\n",
      "\u2713 Dataset ready: 20000 rows\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from Hugging Face\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: Downloading dataset from Hugging Face...\")\n",
    "print(\"=\" * 80)\n",
    "ds = load_dataset(\"jhan21/amazon-food-reviews-dataset\")\n",
    "print(f\"\u2713 Dataset loaded. Available splits: {list(ds.keys())}\")\n",
    "\n",
    "# Convert to pandas DataFrame (the dataset has a 'train' split)\n",
    "print(\"\\nConverting to pandas DataFrame...\")\n",
    "df = ds[\"train\"].to_pandas()\n",
    "print(f\"\u2713 Original dataset size: {len(df)} rows, {len(df.columns)} columns\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "\n",
    "# Keep relevant columns and drop NaNs\n",
    "print(\"\\nFiltering data...\")\n",
    "print(f\"  Before filtering: {len(df)} rows\")\n",
    "df = df[[\"Summary\", \"Text\"]].dropna()\n",
    "print(f\"  After dropping NaN: {len(df)} rows\")\n",
    "\n",
    "# Filter out very long reviews to save memory/time\n",
    "df = df[df[\"Text\"].str.len() <= 512]\n",
    "print(f\"  After filtering long reviews (<=512 chars): {len(df)} rows\")\n",
    "\n",
    "# Sample data for faster training (adjust as needed)\n",
    "SAMPLE_SIZE = 20000\n",
    "if len(df) > SAMPLE_SIZE:\n",
    "    print(f\"\\nSampling {SAMPLE_SIZE} rows from {len(df)} total rows...\")\n",
    "    df = df.sample(SAMPLE_SIZE, random_state=42)\n",
    "    print(f\"\u2713 Sampled dataset size: {len(df)} rows\")\n",
    "else:\n",
    "    print(f\"\\nUsing full dataset: {len(df)} rows\")\n",
    "\n",
    "print(\"\\nSample data preview:\")\n",
    "print(df.head(3))\n",
    "print(f\"\\n\u2713 Dataset ready: {len(df)} rows\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471a479",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0471a479",
    "outputId": "754b9bf2-d5e8-4160-d40f-b00a9e03028b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "STEP 2: Splitting dataset into train/val/test...\n",
      "================================================================================\n",
      "\u2713 Split complete:\n",
      "  Train: 16000 rows (80%)\n",
      "  Validation: 2000 rows (10%)\n",
      "  Test: 2000 rows (10%)\n",
      "\n",
      "\u2713 Datasets created:\n",
      "  Train: 16000 samples\n",
      "  Val: 2000 samples\n",
      "  Test: 2000 samples\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 2: Splitting dataset into train/val/test...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"\u2713 Split complete:\")\n",
    "print(f\"  Train: {len(train_df)} rows (80%)\")\n",
    "print(f\"  Validation: {len(val_df)} rows (10%)\")\n",
    "print(f\"  Test: {len(test_df)} rows (10%)\")\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_ds = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "test_ds = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "print(f\"\\n\u2713 Datasets created:\")\n",
    "print(f\"  Train: {len(train_ds)} samples\")\n",
    "print(f\"  Val: {len(val_ds)} samples\")\n",
    "print(f\"  Test: {len(test_ds)} samples\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b02077",
   "metadata": {
    "id": "e7b02077"
   },
   "source": [
    "## 3. Model and Tokenizer Setup\n",
    "\n",
    "We use `google/flan-t5-small`. We load two copies:\n",
    "1. `base_model`: Keeps original weights to measure baseline performance and forgetting.\n",
    "2. `model`: Will be fine-tuned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5434b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "be5434b2",
    "outputId": "2bc2145a-d978-4ce5-d3f9-766e0f655ed6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "STEP 3: Loading model and tokenizer...\n",
      "================================================================================\n",
      "Model: google/flan-t5-small\n",
      "Device: cuda\n",
      "\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a7de2ba54d44f78881c8210a145081f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "326c9b3cd9e54ea995fd5fb45a56757b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d63cb0a4b5cd49ab93fe17fa55fd23b8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd77ec28211c47dba5e27cae42f39e61"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Tokenizer loaded. Vocab size: 32100\n",
      "  Pad token: 0, EOS token: 1\n",
      "\n",
      "Loading model for fine-tuning (GPU-optimized)...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05cb71824d2446adab6707331fa92a00"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a20e8dc572da4d2294aad25b85b07d78"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d82f7c3b0cc42cdb7f295d89ce04fd8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Model loaded. Parameters: 76,961,152\n",
      "  Model device: cuda:0\n",
      "  Model dtype: torch.float32\n",
      "\n",
      "Loading base model for comparison...\n",
      "\u2713 Base model loaded\n",
      "  Base model device: cuda:0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Model selection based on AI recommendation; see [1]\n",
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 3: Loading model and tokenizer...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(\"\\nLoading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"\u2713 Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"  Pad token: {tokenizer.pad_token_id}, EOS token: {tokenizer.eos_token_id}\")\n",
    "\n",
    "print(\"\\nLoading model for fine-tuning (GPU-optimized)...\")\n",
    "# GPU-OPTIMIZED: Load model directly on GPU with explicit dtype\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,  # Explicit FP32 dtype\n",
    "    device_map=\"auto\"  # Auto-place model on GPU (no CPU intermediate step)\n",
    ")\n",
    "model.train()  # Explicit training mode\n",
    "print(f\"\u2713 Model loaded. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Model device: {next(model.parameters()).device}\")\n",
    "print(f\"  Model dtype: {next(model.parameters()).dtype}\")\n",
    "\n",
    "print(\"\\nLoading base model for comparison...\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "base_model.eval()  # Set to eval mode (won't be trained)\n",
    "print(f\"\u2713 Base model loaded\")\n",
    "print(f\"  Base model device: {next(base_model.parameters()).device}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12bb09",
   "metadata": {
    "id": "ec12bb09"
   },
   "source": [
    "## 4. Tokenization\n",
    "\n",
    "We preprocess the text inputs with a prefix \"Summarize this review: \".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e46b7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "dd3e46b7",
    "outputId": "eff868cd-f497-4c76-d53e-d02deaddbaef"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "STEP 4: Tokenizing datasets...\n",
      "================================================================================\n",
      "Max input length: 256 tokens\n",
      "Max target length: 32 tokens\n",
      "Prefix: 'Summarize this review: '\n",
      "\n",
      "Tokenizing training set...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b252ac1b39fb49938c56cadf457c7423"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Train tokenized: 16000 samples\n",
      "  Columns after tokenization: ['input_ids', 'attention_mask', 'labels']\n",
      "Tokenizing validation set...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c4114be82d84ca094fb75a34663c276"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Val tokenized: 2000 samples\n",
      "Tokenizing test set...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "acaae577ff70438c91f1133ce7de8c8e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 Test tokenized: 2000 samples\n",
      "\n",
      "Example tokenized input:\n",
      "  Input IDs length: 68\n",
      "  Labels length: 4\n",
      "  Decoded input: Summarize this review: This is the second time I purchased this Fancy Feast Chunky Turkey Feast for my finicky cats. This product/flavor is no longer carried in the markets here, so I'm...\n",
      "  Decoded label: Great buy!</s>\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "MAX_INPUT_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 32\n",
    "PREFIX = \"Summarize this review: \"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 4: Tokenizing datasets...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Max input length: {MAX_INPUT_LENGTH} tokens\")\n",
    "print(f\"Max target length: {MAX_TARGET_LENGTH} tokens\")\n",
    "print(f\"Prefix: '{PREFIX}'\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [PREFIX + doc for doc in examples[\"Text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"Summary\"], max_length=MAX_TARGET_LENGTH, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"\\nTokenizing training set...\")\n",
    "tokenized_train = train_ds.map(preprocess_function, batched=True, remove_columns=train_ds.column_names)\n",
    "print(f\"\u2713 Train tokenized: {len(tokenized_train)} samples\")\n",
    "print(f\"  Columns after tokenization: {tokenized_train.column_names}\")\n",
    "\n",
    "print(\"Tokenizing validation set...\")\n",
    "tokenized_val = val_ds.map(preprocess_function, batched=True, remove_columns=val_ds.column_names)\n",
    "print(f\"\u2713 Val tokenized: {len(tokenized_val)} samples\")\n",
    "\n",
    "print(\"Tokenizing test set...\")\n",
    "tokenized_test = test_ds.map(preprocess_function, batched=True, remove_columns=test_ds.column_names)\n",
    "print(f\"\u2713 Test tokenized: {len(tokenized_test)} samples\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample tokenized input:\")\n",
    "example = tokenized_train[0]\n",
    "print(f\"  Input IDs length: {len(example['input_ids'])}\")\n",
    "print(f\"  Labels length: {len(example['labels'])}\")\n",
    "print(f\"  Decoded input: {tokenizer.decode(example['input_ids'][:50])}...\")\n",
    "print(f\"  Decoded label: {tokenizer.decode([l for l in example['labels'] if l != -100])}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff7b90",
   "metadata": {
    "id": "53ff7b90"
   },
   "source": [
    "## 5. Forgetting Analysis (Before Training)\n",
    "\n",
    "We define a small set of general knowledge questions to test the \"forgetting\" hypothesis. We check how well the base model answers them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa42108",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "afa42108",
    "outputId": "409ff82f-8575-4fba-9c84-fd6d15945b0c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "STEP 5: Evaluating Base Model on QA set (Before Training)...\n",
      "================================================================================\n",
      "Number of QA pairs: 8\n",
      "Device: cuda\n",
      "Base model on device: cuda:0\n",
      "--- Forgetting Analysis ---\n",
      "Q: What is the capital of France?\n",
      "  Expected: Paris | Predicted: london | \u2717\n",
      "Q: How many days are in a week?\n",
      "  Expected: 7 | Predicted: 7 days | \u2713\n",
      "Q: What gas do plants absorb?\n",
      "  Expected: carbon dioxide | Predicted: helium | \u2717\n",
      "Q: What is the largest planet in our solar system?\n",
      "  Expected: Jupiter | Predicted: venus | \u2717\n",
      "Q: What is H2O?\n",
      "  Expected: water | Predicted: H2O | \u2717\n",
      "Q: Who wrote Romeo and Juliet?\n",
      "  Expected: Shakespeare | Predicted: edward wilson | \u2717\n",
      "Q: What color is the sky on a clear day?\n",
      "  Expected: blue | Predicted: blue | \u2713\n",
      "Q: What is 2 + 2?\n",
      "  Expected: 4 | Predicted: 2 + 2 | \u2717\n",
      "\n",
      "Accuracy: 25.00% (2/8)\n",
      "\n",
      "\u2713 Base model evaluation complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Forgetting analysis approach based on AI recommendation; see [4]\n",
    "qa_pairs = [\n",
    "    (\"What is the capital of France?\", \"Paris\"),\n",
    "    (\"How many days are in a week?\", \"7\"),\n",
    "    (\"What gas do plants absorb?\", \"carbon dioxide\"),\n",
    "    (\"What is the largest planet in our solar system?\", \"Jupiter\"),\n",
    "    (\"What is H2O?\", \"water\"),\n",
    "    (\"Who wrote Romeo and Juliet?\", \"Shakespeare\"),\n",
    "    (\"What color is the sky on a clear day?\", \"blue\"),\n",
    "    (\"What is 2 + 2?\", \"4\")\n",
    "]\n",
    "\n",
    "def evaluate_forgetting(model_obj, tokenizer_obj, questions, device):\n",
    "    model_obj.eval()\n",
    "    correct = 0\n",
    "    results = []\n",
    "\n",
    "    print(\"--- Forgetting Analysis ---\")\n",
    "    for q, ans in questions:\n",
    "        # FLAN-T5 prompt format based on AI guidance; see [5]\n",
    "        prompt = f\"Question: {q}\\nAnswer:\"\n",
    "        input_ids = tokenizer_obj(prompt, return_tensors=\"pt\", max_length=128, truncation=True).input_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model_obj.generate(\n",
    "                input_ids,\n",
    "                max_length=50,\n",
    "                num_beams=2,\n",
    "                early_stopping=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        pred = tokenizer_obj.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # More flexible answer matching\n",
    "        pred_lower = pred.lower()\n",
    "        ans_lower = ans.lower()\n",
    "\n",
    "        # Check if answer is in prediction (handles partial matches)\n",
    "        is_correct = (\n",
    "            ans_lower in pred_lower or\n",
    "            pred_lower in ans_lower or\n",
    "            any(word in pred_lower for word in ans_lower.split() if len(word) > 2)\n",
    "        )\n",
    "\n",
    "        # Special cases for numeric answers\n",
    "        if ans.isdigit():\n",
    "            # Extract numbers from prediction\n",
    "            import re\n",
    "            numbers = re.findall(r'\\d+', pred)\n",
    "            is_correct = ans in numbers or is_correct\n",
    "\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "\n",
    "        results.append({\"Question\": q, \"Expected\": ans, \"Predicted\": pred, \"Correct\": is_correct})\n",
    "        print(f\"Q: {q}\")\n",
    "        print(f\"  Expected: {ans} | Predicted: {pred} | {'\u2713' if is_correct else '\u2717'}\")\n",
    "\n",
    "    accuracy = correct / len(questions)\n",
    "    print(f\"\\nAccuracy: {accuracy:.2%} ({correct}/{len(questions)})\")\n",
    "    return accuracy, results\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 5: Evaluating Base Model on QA set (Before Training)...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Number of QA pairs: {len(qa_pairs)}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Base model on device: {next(base_model.parameters()).device}\")\n",
    "\n",
    "try:\n",
    "    base_qa_acc, base_qa_results = evaluate_forgetting(base_model, tokenizer, qa_pairs, device)\n",
    "    print(f\"\\n\u2713 Base model evaluation complete!\")\n",
    "    print(\"=\" * 80)\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Base model evaluation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c7c6d",
   "metadata": {
    "id": "191c7c6d"
   },
   "source": [
    "## 6. Fine-Tuning\n",
    "\n",
    "We use `Seq2SeqTrainer` to fine-tune the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf71f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cdcf71f8",
    "outputId": "45e27bbb-c58c-45db-f3c0-d4b9e5338b4c"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0bb3f559a9054366ba1df78a6f4d7b0c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2713 ROUGE metric loaded\n",
      "\n",
      "================================================================================\n",
      "GPU DIAGNOSTICS\n",
      "================================================================================\n",
      "CUDA available: True\n",
      "CUDA version: 12.6\n",
      "PyTorch version: 2.9.0+cu126\n",
      "GPU name: Tesla T4\n",
      "GPU capability: (7, 5)\n",
      "GPU memory: 15.83 GB\n",
      "Supports BF16: True\n",
      "================================================================================\n",
      "\n",
      "Checking model weights for NaN...\n",
      "  \u2713 No NaN in model weights\n",
      "Model device: cuda:0\n",
      "Model dtype: torch.float32\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "DIAGNOSTIC: Inspecting training data batch...\n",
      "================================================================================\n",
      "Batch keys: ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids']\n",
      "Input IDs shape: torch.Size([2, 124])\n",
      "Labels shape: torch.Size([2, 6])\n",
      "\n",
      "Sample input IDs (first 20): [12198, 1635, 1737, 48, 1132, 10, 100, 19, 8, 511, 97, 27, 3907, 48, 377, 6833, 377, 11535, 4004, 6513]\n",
      "Sample labels (first 20): [1651, 805, 55, 1, -100, -100]\n",
      "\n",
      "Label statistics:\n",
      "  Total label positions: 12\n",
      "  Non-ignore labels (not -100): 10\n",
      "  Ignore labels (-100): 2\n",
      "  Ignore ratio: 16.67%\n",
      "\n",
      "================================================================================\n",
      "DIAGNOSTIC: Testing forward pass with sample batch...\n",
      "================================================================================\n",
      "Forward pass loss: 3.075017\n",
      "\u2713 Forward pass loss looks good: 3.075017 - Model should learn!\n",
      "================================================================================\n",
      "================================================================================\n",
      "STEP 6: Setting up training...\n",
      "================================================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-3819739988.py:225: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training configuration:\n",
      "  Epochs: 2\n",
      "  Batch size: 4\n",
      "  Learning rate: 0.0002\n",
      "  Generation max length: 32\n",
      "  Generation num beams: 4\n",
      "  BF16: True (GPU-optimized for modern GPU)\n",
      "  FP16: False\n",
      "  Output dir: ./flan-t5-summarizer\n",
      "\n",
      "Creating trainer...\n",
      "\u2713 Trainer created\n",
      "  Train samples: 16000\n",
      "  Eval samples: 2000\n",
      "\n",
      "================================================================================\n",
      "Starting training...\n",
      "================================================================================\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshaunak1206\u001b[0m (\u001b[33mshaunak\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20251201_224652-3snic0b5</code>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shaunak/huggingface/runs/3snic0b5' target=\"_blank\">fearless-vortex-4</a></strong> to <a href='https://wandb.ai/shaunak/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/shaunak/huggingface' target=\"_blank\">https://wandb.ai/shaunak/huggingface</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/shaunak/huggingface/runs/3snic0b5' target=\"_blank\">https://wandb.ai/shaunak/huggingface/runs/3snic0b5</a>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8000' max='8000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8000/8000 22:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.107000</td>\n",
       "      <td>3.028076</td>\n",
       "      <td>15.888100</td>\n",
       "      <td>5.838300</td>\n",
       "      <td>15.512200</td>\n",
       "      <td>15.555200</td>\n",
       "      <td>7.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.872300</td>\n",
       "      <td>2.997926</td>\n",
       "      <td>17.032500</td>\n",
       "      <td>6.105600</td>\n",
       "      <td>16.684900</td>\n",
       "      <td>16.731900</td>\n",
       "      <td>6.770000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[TRAINING] Step 100: Loss = 3.652800\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000198\n",
      "\n",
      "[TRAINING] Step 200: Loss = 3.444100\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000195\n",
      "\n",
      "[TRAINING] Step 300: Loss = 3.552100\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000193\n",
      "\n",
      "[TRAINING] Step 400: Loss = 3.321600\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000190\n",
      "\n",
      "[TRAINING] Step 500: Loss = 3.409300\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000188\n",
      "\n",
      "[TRAINING] Step 600: Loss = 3.428800\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000185\n",
      "\n",
      "[TRAINING] Step 700: Loss = 3.404900\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000183\n",
      "\n",
      "[TRAINING] Step 800: Loss = 3.320700\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000180\n",
      "\n",
      "[TRAINING] Step 900: Loss = 3.327500\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000178\n",
      "\n",
      "[TRAINING] Step 1000: Loss = 3.424200\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000175\n",
      "\n",
      "[TRAINING] Step 1100: Loss = 3.454100\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000173\n",
      "\n",
      "[TRAINING] Step 1200: Loss = 3.225000\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000170\n",
      "\n",
      "[TRAINING] Step 1300: Loss = 3.391000\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000168\n",
      "\n",
      "[TRAINING] Step 1400: Loss = 3.426600\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000165\n",
      "\n",
      "[TRAINING] Step 1500: Loss = 3.179300\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000163\n",
      "\n",
      "[TRAINING] Step 1600: Loss = 3.419800\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000160\n",
      "\n",
      "[TRAINING] Step 1700: Loss = 3.392700\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000158\n",
      "\n",
      "[TRAINING] Step 1800: Loss = 3.303600\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000155\n",
      "\n",
      "[TRAINING] Step 1900: Loss = 3.230500\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000153\n",
      "\n",
      "[TRAINING] Step 2000: Loss = 3.387900\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000150\n",
      "\n",
      "[TRAINING] Step 2100: Loss = 3.255300\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000148\n",
      "\n",
      "[TRAINING] Step 2200: Loss = 3.318800\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000145\n",
      "\n",
      "[TRAINING] Step 2300: Loss = 3.270300\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000143\n",
      "\n",
      "[TRAINING] Step 2400: Loss = 3.257400\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000140\n",
      "\n",
      "[TRAINING] Step 2500: Loss = 3.293900\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000138\n",
      "\n",
      "[TRAINING] Step 2600: Loss = 3.277800\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000135\n",
      "\n",
      "[TRAINING] Step 2700: Loss = 3.207800\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000133\n",
      "\n",
      "[TRAINING] Step 2800: Loss = 3.329700\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000130\n",
      "\n",
      "[TRAINING] Step 2900: Loss = 3.179900\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000128\n",
      "\n",
      "[TRAINING] Step 3000: Loss = 3.235600\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000125\n",
      "\n",
      "[TRAINING] Step 3100: Loss = 3.419400\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000123\n",
      "\n",
      "[TRAINING] Step 3200: Loss = 3.204400\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000120\n",
      "\n",
      "[TRAINING] Step 3300: Loss = 3.069700\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000118\n",
      "\n",
      "[TRAINING] Step 3400: Loss = 3.243300\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000115\n",
      "\n",
      "[TRAINING] Step 3500: Loss = 3.139200\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000113\n",
      "\n",
      "[TRAINING] Step 3600: Loss = 3.187600\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000110\n",
      "\n",
      "[TRAINING] Step 3700: Loss = 3.184300\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000108\n",
      "\n",
      "[TRAINING] Step 3800: Loss = 3.240600\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000105\n",
      "\n",
      "[TRAINING] Step 3900: Loss = 3.225900\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000103\n",
      "\n",
      "[TRAINING] Step 4000: Loss = 3.107000\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000100\n",
      "\n",
      "[DEBUG] compute_metrics called - predictions shape: (2000, 32), labels shape: (2000, 32)\n",
      "[DEBUG] Predictions clipped to vocab range [0, 32099]\n",
      "[DEBUG] Decoded 2000 predictions\n",
      "[DEBUG] Decoded 2000 labels\n",
      "[DEBUG] ROUGE computed: {'rouge1': np.float64(0.15888098466792272), 'rouge2': np.float64(0.05838312425087612), 'rougeL': np.float64(0.1551224127004572), 'rougeLsum': np.float64(0.1555524527411199)}\n",
      "[DEBUG] Average generation length: 7.43\n",
      "[DEBUG] Final metrics: {'rouge1': np.float64(15.8881), 'rouge2': np.float64(5.8383), 'rougeL': np.float64(15.5122), 'rougeLsum': np.float64(15.5552), 'gen_len': np.float64(7.43)}\n",
      "\n",
      "[TRAINING] Step 4100: Loss = 3.004400\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000098\n",
      "\n",
      "[TRAINING] Step 4200: Loss = 2.925400\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000095\n",
      "\n",
      "[TRAINING] Step 4300: Loss = 2.959700\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000093\n",
      "\n",
      "[TRAINING] Step 4400: Loss = 2.940600\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000090\n",
      "\n",
      "[TRAINING] Step 4500: Loss = 2.817500\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000088\n",
      "\n",
      "[TRAINING] Step 4600: Loss = 2.931300\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000085\n",
      "\n",
      "[TRAINING] Step 4700: Loss = 2.973700\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000083\n",
      "\n",
      "[TRAINING] Step 4800: Loss = 2.874200\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000080\n",
      "\n",
      "[TRAINING] Step 4900: Loss = 2.803000\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000078\n",
      "\n",
      "[TRAINING] Step 5000: Loss = 2.874900\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000075\n",
      "\n",
      "[TRAINING] Step 5100: Loss = 2.802000\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000073\n",
      "\n",
      "[TRAINING] Step 5200: Loss = 2.907000\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000070\n",
      "\n",
      "[TRAINING] Step 5300: Loss = 2.976300\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000068\n",
      "\n",
      "[TRAINING] Step 5400: Loss = 2.886700\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000065\n",
      "\n",
      "[TRAINING] Step 5500: Loss = 2.823800\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000063\n",
      "\n",
      "[TRAINING] Step 5600: Loss = 2.985800\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000060\n",
      "\n",
      "[TRAINING] Step 5700: Loss = 3.000800\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000058\n",
      "\n",
      "[TRAINING] Step 5800: Loss = 2.775900\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000055\n",
      "\n",
      "[TRAINING] Step 5900: Loss = 2.972900\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000053\n",
      "\n",
      "[TRAINING] Step 6000: Loss = 2.861700\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000050\n",
      "\n",
      "[TRAINING] Step 6100: Loss = 2.825600\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000048\n",
      "\n",
      "[TRAINING] Step 6200: Loss = 2.885900\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000045\n",
      "\n",
      "[TRAINING] Step 6300: Loss = 2.787100\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000043\n",
      "\n",
      "[TRAINING] Step 6400: Loss = 2.926400\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000040\n",
      "\n",
      "[TRAINING] Step 6500: Loss = 2.888700\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000038\n",
      "\n",
      "[TRAINING] Step 6600: Loss = 2.904000\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000035\n",
      "\n",
      "[TRAINING] Step 6700: Loss = 2.858600\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000033\n",
      "\n",
      "[TRAINING] Step 6800: Loss = 2.964800\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000030\n",
      "\n",
      "[TRAINING] Step 6900: Loss = 2.976600\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000028\n",
      "\n",
      "[TRAINING] Step 7000: Loss = 2.744800\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000025\n",
      "\n",
      "[TRAINING] Step 7100: Loss = 2.918500\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000023\n",
      "\n",
      "[TRAINING] Step 7200: Loss = 2.996600\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000020\n",
      "\n",
      "[TRAINING] Step 7300: Loss = 2.765200\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000018\n",
      "\n",
      "[TRAINING] Step 7400: Loss = 3.005100\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000015\n",
      "\n",
      "[TRAINING] Step 7500: Loss = 2.949400\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000013\n",
      "\n",
      "[TRAINING] Step 7600: Loss = 2.890200\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000010\n",
      "\n",
      "[TRAINING] Step 7700: Loss = 2.857600\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000008\n",
      "\n",
      "[TRAINING] Step 7800: Loss = 2.886400\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000005\n",
      "\n",
      "[TRAINING] Step 7900: Loss = 2.871100\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000003\n",
      "\n",
      "[TRAINING] Step 8000: Loss = 2.872300\n",
      "  \u2713 Loss looks good - model is learning!\n",
      "[TRAINING] Learning Rate = 0.000000\n",
      "\n",
      "[DEBUG] compute_metrics called - predictions shape: (2000, 32), labels shape: (2000, 32)\n",
      "[DEBUG] Predictions clipped to vocab range [0, 32099]\n",
      "[DEBUG] Decoded 2000 predictions\n",
      "[DEBUG] Decoded 2000 labels\n",
      "[DEBUG] ROUGE computed: {'rouge1': np.float64(0.1703251476761794), 'rouge2': np.float64(0.06105645255973202), 'rougeL': np.float64(0.16684890198164015), 'rougeLsum': np.float64(0.16731850675813098)}\n",
      "[DEBUG] Average generation length: 6.77\n",
      "[DEBUG] Final metrics: {'rouge1': np.float64(17.0325), 'rouge2': np.float64(6.1056), 'rougeL': np.float64(16.6849), 'rougeLsum': np.float64(16.7319), 'gen_len': np.float64(6.77)}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "================================================================================\n",
      "\u2713 Training completed successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ROUGE metric implementation based on AI guidance; see [2]\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "print(\"\u2713 ROUGE metric loaded\")\n",
    "\n",
    "# GPU DIAGNOSTICS - Check environment before training\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GPU DIAGNOSTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU capability: {torch.cuda.get_device_capability(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Supports BF16: {torch.cuda.get_device_capability()[0] >= 7}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check model for NaN weights\n",
    "print(\"\\nChecking model weights for NaN...\")\n",
    "has_nan = False\n",
    "nan_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if torch.isnan(param).any():\n",
    "        print(f\"  \u274c NaN found in: {name}\")\n",
    "        has_nan = True\n",
    "        nan_params.append(name)\n",
    "if not has_nan:\n",
    "    print(\"  \u2713 No NaN in model weights\")\n",
    "else:\n",
    "    print(f\"\\n  \u26a0\ufe0f  WARNING: Found NaN in {len(nan_params)} parameters!\")\n",
    "    print(f\"  This will cause training to fail. Model needs to be reloaded.\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        print(f\"\\n[DEBUG] compute_metrics called - predictions shape: {np.array(predictions).shape}, labels shape: {np.array(labels).shape}\")\n",
    "\n",
    "        # Convert to numpy if needed and ensure valid token IDs\n",
    "        predictions = np.array(predictions)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        # Clip predictions to valid token ID range (0 to vocab_size-1) - fix for OverflowError; see [3]\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        predictions = np.clip(predictions, 0, vocab_size - 1)\n",
    "        print(f\"[DEBUG] Predictions clipped to vocab range [0, {vocab_size-1}]\")\n",
    "\n",
    "        # Decode predictions\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        print(f\"[DEBUG] Decoded {len(decoded_preds)} predictions\")\n",
    "\n",
    "        # Replace -100 (ignored labels) with pad_token_id for decoding\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        labels = np.clip(labels, 0, vocab_size - 1)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        print(f\"[DEBUG] Decoded {len(decoded_labels)} labels\")\n",
    "\n",
    "        # Compute ROUGE\n",
    "        result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        print(f\"[DEBUG] ROUGE computed: {result}\")\n",
    "\n",
    "        # Calculate actual generation length (only count non-padding tokens up to EOS)\n",
    "        gen_lens = []\n",
    "        for pred in predictions:\n",
    "            # Find EOS token or count non-padding tokens\n",
    "            pred_list = pred.tolist() if hasattr(pred, 'tolist') else list(pred)\n",
    "            # Remove padding tokens (0) and count until EOS (1 for T5)\n",
    "            length = 0\n",
    "            for token_id in pred_list:\n",
    "                if token_id == tokenizer.eos_token_id or token_id == 1:  # EOS token\n",
    "                    break\n",
    "                if token_id != tokenizer.pad_token_id and token_id != 0:\n",
    "                    length += 1\n",
    "            gen_lens.append(length)\n",
    "\n",
    "        avg_gen_len = np.mean(gen_lens) if gen_lens else 0\n",
    "        result[\"gen_len\"] = avg_gen_len\n",
    "        print(f\"[DEBUG] Average generation length: {avg_gen_len:.2f}\")\n",
    "\n",
    "        # Convert ROUGE scores to percentages (but NOT gen_len)\n",
    "        final_result = {}\n",
    "        for k, v in result.items():\n",
    "            if k == \"gen_len\":\n",
    "                final_result[k] = round(v, 2)  # Keep gen_len as-is, just round\n",
    "            else:\n",
    "                final_result[k] = round(v * 100, 4)  # Convert ROUGE to percentage\n",
    "\n",
    "        print(f\"[DEBUG] Final metrics: {final_result}\")\n",
    "        return final_result\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] compute_metrics failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0, \"gen_len\": 0.0}\n",
    "\n",
    "# DEBUG: Let's inspect a batch to see what's being fed to the model\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DIAGNOSTIC: Inspecting training data batch...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Get a small batch\n",
    "sample_batch = [tokenized_train[i] for i in range(2)]\n",
    "collated_batch = data_collator(sample_batch)\n",
    "\n",
    "print(f\"Batch keys: {list(collated_batch.keys())}\")\n",
    "print(f\"Input IDs shape: {collated_batch['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {collated_batch['labels'].shape}\")\n",
    "print(f\"\\nSample input IDs (first 20): {collated_batch['input_ids'][0][:20].tolist()}\")\n",
    "print(f\"Sample labels (first 20): {collated_batch['labels'][0][:20].tolist()}\")\n",
    "\n",
    "# Count how many labels are NOT -100 (i.e., actual labels vs padding)\n",
    "labels_array = collated_batch['labels'].numpy()\n",
    "non_ignore_labels = np.sum(labels_array != -100)\n",
    "total_labels = labels_array.size\n",
    "ignore_ratio = (total_labels - non_ignore_labels) / total_labels\n",
    "\n",
    "print(f\"\\nLabel statistics:\")\n",
    "print(f\"  Total label positions: {total_labels}\")\n",
    "print(f\"  Non-ignore labels (not -100): {non_ignore_labels}\")\n",
    "print(f\"  Ignore labels (-100): {total_labels - non_ignore_labels}\")\n",
    "print(f\"  Ignore ratio: {ignore_ratio:.2%}\")\n",
    "\n",
    "if ignore_ratio > 0.95:\n",
    "    print(\"\\n\u26a0\ufe0f  WARNING: More than 95% of labels are -100 (ignore)!\")\n",
    "    print(\"   This will cause very low or zero loss!\")\n",
    "\n",
    "# Try a forward pass to see actual loss\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DIAGNOSTIC: Testing forward pass with sample batch...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Move batch to device\n",
    "    batch_device = {k: v.to(device) for k, v in collated_batch.items()}\n",
    "    outputs = model(**batch_device)\n",
    "    loss = outputs.loss\n",
    "    loss_value = loss.item()\n",
    "    print(f\"Forward pass loss: {loss_value:.6f}\")\n",
    "\n",
    "    if np.isnan(loss_value):\n",
    "        print(\"\\n\u26a0\ufe0f  CRITICAL: Forward pass loss is NaN!\")\n",
    "        print(\"   This indicates a GPU-specific numerical instability issue.\")\n",
    "        print(f\"   Model device: {next(model.parameters()).device}\")\n",
    "        print(f\"   Batch device: {batch_device['input_ids'].device}\")\n",
    "        print(f\"   Model dtype: {next(model.parameters()).dtype}\")\n",
    "        print(\"\\n   SOLUTION: Will enable BF16 training for GPU compatibility.\")\n",
    "    elif loss_value == 0.0:\n",
    "        print(\"\\n\u26a0\ufe0f  CRITICAL: Forward pass loss is 0.0!\")\n",
    "        print(\"   This confirms the model is not computing loss correctly.\")\n",
    "        print(\"   The issue is likely with label preparation.\")\n",
    "    else:\n",
    "        print(f\"\u2713 Forward pass loss looks good: {loss_value:.6f} - Model should learn!\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Custom callback to print training loss in real-time\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class LossLoggingCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            # Print training loss if available\n",
    "            if \"loss\" in logs:\n",
    "                step = state.global_step\n",
    "                loss = logs[\"loss\"]\n",
    "                print(f\"\\n[TRAINING] Step {step}: Loss = {loss:.6f}\")\n",
    "                # If loss is 0 or NaN, print a warning\n",
    "                if np.isnan(loss):\n",
    "                    print(\"  \u26a0\ufe0f  WARNING: Loss is NaN - training has a problem!\")\n",
    "                elif loss == 0.0:\n",
    "                    print(\"  \u26a0\ufe0f  WARNING: Loss is 0.0 - model may not be learning!\")\n",
    "                elif loss > 0.01:\n",
    "                    print(\"  \u2713 Loss looks good - model is learning!\")\n",
    "\n",
    "            # Print learning rate if available\n",
    "            if \"learning_rate\" in logs:\n",
    "                lr = logs[\"learning_rate\"]\n",
    "                print(f\"[TRAINING] Learning Rate = {lr:.6f}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 6: Setting up training...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# GPU-optimized training arguments with BF16 support\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 7\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan-t5-summarizer\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    generation_num_beams=4,\n",
    "    bf16=use_bf16,  # Use BF16 for modern GPUs (fixes NaN loss!)\n",
    "    fp16=False,  # Don't use FP16\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge1\",\n",
    ")\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {args.learning_rate}\")\n",
    "print(f\"  Generation max length: {args.generation_max_length}\")\n",
    "print(f\"  Generation num beams: {args.generation_num_beams}\")\n",
    "print(f\"  BF16: {args.bf16} (GPU-optimized for {'modern' if use_bf16 else 'older'} GPU)\")\n",
    "print(f\"  FP16: {args.fp16}\")\n",
    "print(f\"  Output dir: {args.output_dir}\")\n",
    "\n",
    "print(\"\\nCreating trainer...\")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[LossLoggingCallback()],\n",
    ")\n",
    "print(f\"\u2713 Trainer created\")\n",
    "print(f\"  Train samples: {len(tokenized_train)}\")\n",
    "print(f\"  Eval samples: {len(tokenized_val)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"\u2713 Training completed successfully!\")\n",
    "    print(\"=\" * 80)\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36373d51",
   "metadata": {
    "id": "36373d51"
   },
   "source": [
    "## 7. Evaluation: Summarization Quality\n",
    "\n",
    "Compare ROUGE scores and look at qualitative examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603b760",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "e603b760",
    "outputId": "ceea6beb-98bc-4464-b70d-6f5dedac2e6f"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "STEP 7: Evaluating on test set...\n",
      "================================================================================\n",
      "Test samples: 2000\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 02:46]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[DEBUG] compute_metrics called - predictions shape: (2000, 32), labels shape: (2000, 32)\n",
      "[DEBUG] Predictions clipped to vocab range [0, 32099]\n",
      "[DEBUG] Decoded 2000 predictions\n",
      "[DEBUG] Decoded 2000 labels\n",
      "[DEBUG] ROUGE computed: {'rouge1': np.float64(0.17236512279656996), 'rouge2': np.float64(0.07056816339276491), 'rougeL': np.float64(0.16915911774918535), 'rougeLsum': np.float64(0.16872712370953644)}\n",
      "[DEBUG] Average generation length: 6.62\n",
      "[DEBUG] Final metrics: {'rouge1': np.float64(17.2365), 'rouge2': np.float64(7.0568), 'rougeL': np.float64(16.9159), 'rougeLsum': np.float64(16.8727), 'gen_len': np.float64(6.62)}\n",
      "\n",
      "\u2713 Test evaluation complete!\n",
      "\n",
      "Test Results:\n",
      "  eval_loss: 2.9697\n",
      "  eval_rouge1: 17.2365\n",
      "  eval_rouge2: 7.0568\n",
      "  eval_rougeL: 16.9159\n",
      "  eval_rougeLsum: 16.8727\n",
      "  eval_gen_len: 6.6200\n",
      "  eval_runtime: 168.0712\n",
      "  eval_samples_per_second: 11.9000\n",
      "  eval_steps_per_second: 2.9750\n",
      "  epoch: 2.0000\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STEP 7: Evaluating on test set...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Test samples: {len(tokenized_test)}\")\n",
    "\n",
    "try:\n",
    "    test_results = trainer.evaluate(tokenized_test)\n",
    "    print(\"\\n\u2713 Test evaluation complete!\")\n",
    "    print(\"\\nTest Results:\")\n",
    "    for key, value in test_results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    print(\"=\" * 80)\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Test evaluation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "AIs5csDIW_7N"
   },
   "id": "AIs5csDIW_7N",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf18f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32cf18f6",
    "outputId": "330e3a0a-b9a9-4d08-894e-3e861fed9b98"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "STEP 8: Qualitative Comparison...\n",
      "================================================================================\n",
      "Comparing 5 examples from test set...\n",
      "================================================================================\n",
      "\n",
      "Example 1/5 (Index 0):\n",
      "  Generating base model summary...\n",
      "  Generating fine-tuned model summary...\n",
      "\n",
      "  Review: This chip has a little tomato taste but the jalepeno seems mild. Good alternative to plain tortilla chips. Not overwhelmed by the taste but a good change....\n",
      "  Reference: Good snack, not very hot\n",
      "  Base Model: This chip has a little tomato taste but the jalepeno seems mild. Good alternative to plain tortilla chips. Not overwhelmed by the taste but \n",
      "  Fine-Tuned: A little tomato taste but a little mild\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 2/5 (Index 5):\n",
      "  Generating base model summary...\n",
      "  Generating fine-tuned model summary...\n",
      "\n",
      "  Review: I received my first jar as a gift and promptly fell in love with it! When I found it on this website I was so thrilled.  The transaction went very smoothly.  Not a problem to be found.  I think the pr...\n",
      "  Reference: Best Jam Ever\n",
      "  Base Model: Love it!\n",
      "  Fine-Tuned: Yummy!\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 3/5 (Index 10):\n",
      "  Generating base model summary...\n",
      "  Generating fine-tuned model summary...\n",
      "\n",
      "  Review: I love it! Great smell and exceptional taste! I've tried lot of different sorts/tastes of coffee from cheap to exclusively expensive, but this one remains one of my faorite for years.<br />Great Coffe...\n",
      "  Reference: Melitta French Vanilla Ground Coffee\n",
      "  Base Model: Five Stars\n",
      "  Fine-Tuned: Love this coffee!\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 4/5 (Index 15):\n",
      "  Generating base model summary...\n",
      "  Generating fine-tuned model summary...\n",
      "\n",
      "  Review: This product does not taste as sweet as the individual packets made by this same company.  Unfortunately they donot sell/package individual packets anymore.  So I just add Irish Cream creamer to it to...\n",
      "  Reference: Cappuchino powder\n",
      "  Base Model: This product does not taste as sweet as the individual packets made by this same company. Unfortunately they do not sell/package individual packets anymore.\n",
      "  Fine-Tuned: Not as sweet as the individual packets made by this company\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 5/5 (Index 20):\n",
      "  Generating base model summary...\n",
      "  Generating fine-tuned model summary...\n",
      "\n",
      "  Review: Our australian shepard puppies love these! and they are the perfect size for training and to put in their treat balls....\n",
      "  Reference: GREAT\n",
      "  Base Model: Great for training!\n",
      "  Fine-Tuned: Great for training!\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\u2713 Qualitative comparison complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Qualitative Comparison\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 8: Qualitative Comparison...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def generate_summary(model_obj, text, device):\n",
    "    try:\n",
    "        inputs = tokenizer(PREFIX + text, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True).to(device)\n",
    "        outputs = model_obj.generate(inputs.input_ids, max_length=MAX_TARGET_LENGTH, num_beams=4)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] generate_summary failed: {e}\")\n",
    "        return f\"[ERROR: {str(e)}]\"\n",
    "\n",
    "sample_indices = [0, 5, 10, 15, 20]\n",
    "print(f\"Comparing {len(sample_indices)} examples from test set...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    try:\n",
    "        print(f\"\\nExample {i}/{len(sample_indices)} (Index {idx}):\")\n",
    "        example = test_ds[idx]\n",
    "        text = example[\"Text\"]\n",
    "        ref_summary = example[\"Summary\"]\n",
    "\n",
    "        print(f\"  Generating base model summary...\")\n",
    "        base_summary = generate_summary(base_model, text, device)\n",
    "\n",
    "        print(f\"  Generating fine-tuned model summary...\")\n",
    "        ft_summary = generate_summary(model, text, device)\n",
    "\n",
    "        print(f\"\\n  Review: {text[:200]}...\")\n",
    "        print(f\"  Reference: {ref_summary}\")\n",
    "        print(f\"  Base Model: {base_summary}\")\n",
    "        print(f\"  Fine-Tuned: {ft_summary}\")\n",
    "        print(\"-\" * 80)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process example {idx}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\u2713 Qualitative comparison complete!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3407042",
   "metadata": {
    "id": "d3407042"
   },
   "source": [
    "## 8. Forgetting Analysis (After Training)\n",
    "\n",
    "Check if the fine-tuned model has forgotten general knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36285762",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36285762",
    "outputId": "42155258-f543-48d1-ed28-b67077611cfb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "STEP 9: Evaluating Fine-Tuned Model on QA set (After Training)...\n",
      "================================================================================\n",
      "Number of QA pairs: 8\n",
      "Device: cuda\n",
      "Fine-tuned model on device: cuda:0\n",
      "--- Forgetting Analysis ---\n",
      "Q: What is the capital of France?\n",
      "  Expected: Paris | Predicted: French capital | \u2717\n",
      "Q: How many days are in a week?\n",
      "  Expected: 7 | Predicted: 7 days | \u2713\n",
      "Q: What gas do plants absorb?\n",
      "  Expected: carbon dioxide | Predicted: gas | \u2717\n",
      "Q: What is the largest planet in our solar system?\n",
      "  Expected: Jupiter | Predicted: Earth | \u2717\n",
      "Q: What is H2O?\n",
      "  Expected: water | Predicted: H2O | \u2717\n",
      "Q: Who wrote Romeo and Juliet?\n",
      "  Expected: Shakespeare | Predicted: edmund wilson | \u2717\n",
      "Q: What color is the sky on a clear day?\n",
      "  Expected: blue | Predicted: blue sky | \u2713\n",
      "Q: What is 2 + 2?\n",
      "  Expected: 4 | Predicted: 2 + 2 | \u2717\n",
      "\n",
      "Accuracy: 25.00% (2/8)\n",
      "\n",
      "\u2713 Fine-tuned model evaluation complete!\n",
      "\n",
      "================================================================================\n",
      "FORGETTING ANALYSIS SUMMARY\n",
      "================================================================================\n",
      "Base Model QA Accuracy: 25.00% (2/8)\n",
      "Fine-Tuned Model QA Accuracy: 25.00% (2/8)\n",
      "Change in Accuracy: +0.00%\n",
      "\u2192 No change in general knowledge performance.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STEP 9: Evaluating Fine-Tuned Model on QA set (After Training)...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Number of QA pairs: {len(qa_pairs)}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Fine-tuned model on device: {next(model.parameters()).device}\")\n",
    "\n",
    "try:\n",
    "    ft_qa_acc, ft_qa_results = evaluate_forgetting(model, tokenizer, qa_pairs, device)\n",
    "    print(f\"\\n\u2713 Fine-tuned model evaluation complete!\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FORGETTING ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Base Model QA Accuracy: {base_qa_acc:.2%} ({base_qa_acc * len(qa_pairs):.0f}/{len(qa_pairs)})\")\n",
    "    print(f\"Fine-Tuned Model QA Accuracy: {ft_qa_acc:.2%} ({ft_qa_acc * len(qa_pairs):.0f}/{len(qa_pairs)})\")\n",
    "\n",
    "    diff = ft_qa_acc - base_qa_acc\n",
    "    print(f\"Change in Accuracy: {diff:+.2%}\")\n",
    "\n",
    "    if diff < 0:\n",
    "        print(f\"\u26a0\ufe0f  Forgetting detected! Model lost {abs(diff):.2%} accuracy on general knowledge.\")\n",
    "    elif diff > 0:\n",
    "        print(f\"\u2713 Model improved by {diff:.2%} (unexpected but good!)\")\n",
    "    else:\n",
    "        print(f\"\u2192 No change in general knowledge performance.\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Fine-tuned model evaluation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd78ec",
   "metadata": {
    "id": "bebd78ec"
   },
   "source": [
    "## 9. Save Model\n",
    "\n",
    "Save the fine-tuned model to be downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0503d04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0503d04",
    "outputId": "9fbd6fe1-43b9-4b20-e947-e264d47f9958"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model saved to ./finetuned_summarizer_final\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./finetuned_summarizer_final\")\n",
    "tokenizer.save_pretrained(\"./finetuned_summarizer_final\")\n",
    "\n",
    "print(\"Model saved to ./finetuned_summarizer_final\")\n",
    "# To download from Colab:\n",
    "# from google.colab import files\n",
    "# !zip -r model.zip ./finetuned_summarizer_final\n",
    "# files.download('model.zip')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU",
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}