{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659f2a7a",
   "metadata": {},
   "source": [
    "# Project 3: Fine-Tuning FLAN-T5 for Summarization & Measuring Forgetting\n",
    "\n",
    "**Authors:** Shaunak Kapur & Pranav Krishnan\n",
    "\n",
    "This notebook implements the Project 3 proposal: fine-tuning a small language model (`google/flan-t5-small`) on the Amazon Fine Food Reviews dataset to generate product review summaries. It also evaluates \"forgetting\" by checking the model's performance on a set of general knowledge questions before and after fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50a91d",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Installing required libraries: `transformers`, `datasets`, `evaluate`, `rouge_score`, `accelerate`, `sentencepiece`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d22324",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate rouge_score accelerate sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# The code below was generated by AI; see [2].\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1e698",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data\n",
    "\n",
    "We use the Amazon Fine Food Reviews dataset from Hugging Face. The dataset will be automatically downloaded using `load_dataset`.\n",
    "\n",
    "We will:\n",
    "1. Download the dataset from Hugging Face.\n",
    "2. Convert to pandas DataFrame.\n",
    "3. Drop rows with missing values.\n",
    "4. Sample the data (e.g., 20,000 rows) to keep training time reasonable.\n",
    "5. Split into Train (80%), Validation (10%), and Test (10%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db864825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face\n",
    "# The code below was generated by AI; see [2].\n",
    "print(\"Downloading dataset from Hugging Face...\")\n",
    "ds = load_dataset(\"jhan21/amazon-food-reviews-dataset\")\n",
    "\n",
    "# Convert to pandas DataFrame (the dataset has a 'train' split)\n",
    "df = ds[\"train\"].to_pandas()\n",
    "\n",
    "# Keep relevant columns and drop NaNs\n",
    "df = df[[\"Summary\", \"Text\"]].dropna()\n",
    "\n",
    "# Filter out very long reviews to save memory/time\n",
    "df = df[df[\"Text\"].str.len() <= 512]\n",
    "\n",
    "# Sample data for faster training (adjust as needed)\n",
    "SAMPLE_SIZE = 20000\n",
    "if len(df) > SAMPLE_SIZE:\n",
    "    df = df.sample(SAMPLE_SIZE, random_state=42)\n",
    "\n",
    "print(f\"Dataset size: {len(df)}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The code below was generated by AI; see [2].\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_ds = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "test_ds = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b02077",
   "metadata": {},
   "source": [
    "## 3. Model and Tokenizer Setup\n",
    "\n",
    "We use `google/flan-t5-small`. We load two copies:\n",
    "1. `base_model`: Keeps original weights to measure baseline performance and forgetting.\n",
    "2. `model`: Will be fine-tuned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5434b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Model to be fine-tuned\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Base model for comparison (frozen)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "base_model.to(device)\n",
    "print(\"Models loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12bb09",
   "metadata": {},
   "source": [
    "## 4. Tokenization\n",
    "\n",
    "We preprocess the text inputs with a prefix \"Summarize this review: \".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 32\n",
    "PREFIX = \"Summarize this review: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [PREFIX + doc for doc in examples[\"Text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"Summary\"], max_length=MAX_TARGET_LENGTH, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = train_ds.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_ds.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_ds.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff7b90",
   "metadata": {},
   "source": [
    "## 5. Forgetting Analysis (Before Training)\n",
    "\n",
    "We define a small set of general knowledge questions to test the \"forgetting\" hypothesis. We check how well the base model answers them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa42108",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = [\n",
    "    (\"What is the capital of France?\", \"Paris\"),\n",
    "    (\"How many days are in a week?\", \"7\"),\n",
    "    (\"What gas do plants absorb?\", \"carbon dioxide\"),\n",
    "    (\"What is the largest planet in our solar system?\", \"Jupiter\"),\n",
    "    (\"What is H2O?\", \"water\"),\n",
    "    (\"Who wrote Romeo and Juliet?\", \"Shakespeare\"),\n",
    "    (\"What color is the sky on a clear day?\", \"blue\"),\n",
    "    (\"What is 2 + 2?\", \"4\")\n",
    "]\n",
    "\n",
    "def evaluate_forgetting(model_obj, tokenizer_obj, questions, device):\n",
    "    model_obj.eval()\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    print(\"--- Forgetting Analysis ---\")\n",
    "    for q, ans in questions:\n",
    "        input_ids = tokenizer_obj(\"Answer the question: \" + q, return_tensors=\"pt\").input_ids.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model_obj.generate(input_ids, max_length=20)\n",
    "        \n",
    "        pred = tokenizer_obj.decode(outputs[0], skip_special_tokens=True)\n",
    "        is_correct = ans.lower() in pred.lower()\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            \n",
    "        results.append({\"Question\": q, \"Expected\": ans, \"Predicted\": pred, \"Correct\": is_correct})\n",
    "        print(f\"Q: {q} | Pred: {pred} | Expected: {ans}\")\n",
    "    \n",
    "    accuracy = correct / len(questions)\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    return accuracy, results\n",
    "\n",
    "print(\"Evaluating Base Model on QA set...\")\n",
    "base_qa_acc, base_qa_results = evaluate_forgetting(base_model, tokenizer, qa_pairs, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c7c6d",
   "metadata": {},
   "source": [
    "## 6. Fine-Tuning\n",
    "\n",
    "We use `Seq2SeqTrainer` to fine-tune the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan-t5-summarizer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=(device == \"cuda\"),\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# The code below was generated by AI; see [2].\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36373d51",
   "metadata": {},
   "source": [
    "## 7. Evaluation: Summarization Quality\n",
    "\n",
    "Compare ROUGE scores and look at qualitative examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on Test Set...\")\n",
    "test_results = trainer.evaluate(tokenized_test)\n",
    "print(test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative Comparison\n",
    "def generate_summary(model_obj, text, device):\n",
    "    inputs = tokenizer(PREFIX + text, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True).to(device)\n",
    "    outputs = model_obj.generate(inputs.input_ids, max_length=MAX_TARGET_LENGTH, num_beams=4)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "sample_indices = [0, 5, 10, 15, 20]\n",
    "print(\"--- Qualitative Results ---\\n\")\n",
    "\n",
    "for idx in sample_indices:\n",
    "    example = test_ds[idx]\n",
    "    text = example[\"Text\"]\n",
    "    ref_summary = example[\"Summary\"]\n",
    "    \n",
    "    base_summary = generate_summary(base_model, text, device)\n",
    "    ft_summary = generate_summary(model, text, device)\n",
    "    \n",
    "    print(f\"Review: {text[:200]}...\")\n",
    "    print(f\"Reference: {ref_summary}\")\n",
    "    print(f\"Base Model: {base_summary}\")\n",
    "    print(f\"Fine-Tuned: {ft_summary}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3407042",
   "metadata": {},
   "source": [
    "## 8. Forgetting Analysis (After Training)\n",
    "\n",
    "Check if the fine-tuned model has forgotten general knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36285762",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Fine-Tuned Model on QA set...\")\n",
    "ft_qa_acc, ft_qa_results = evaluate_forgetting(model, tokenizer, qa_pairs, device)\n",
    "\n",
    "print(f\"\\nBase Model QA Accuracy: {base_qa_acc:.2%}\")\n",
    "print(f\"Fine-Tuned Model QA Accuracy: {ft_qa_acc:.2%}\")\n",
    "\n",
    "diff = ft_qa_acc - base_qa_acc\n",
    "print(f\"Change in Accuracy: {diff:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd78ec",
   "metadata": {},
   "source": [
    "## 9. Save Model\n",
    "\n",
    "Save the fine-tuned model to be downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0503d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./finetuned_summarizer_final\")\n",
    "tokenizer.save_pretrained(\"./finetuned_summarizer_final\")\n",
    "\n",
    "print(\"Model saved to ./finetuned_summarizer_final\")\n",
    "# To download from Colab:\n",
    "# from google.colab import files\n",
    "# !zip -r model.zip ./finetuned_summarizer_final\n",
    "# files.download('model.zip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f3bfb3",
   "metadata": {},
   "source": [
    "# Project 3: Fine-Tuning FLAN-T5 for Summarization & Measuring Forgetting\n",
    "\n",
    "**Authors:** Shaunak Kapur & Pranav Krishnan\n",
    "\n",
    "This notebook implements the Project 3 proposal: fine-tuning a small language model (`google/flan-t5-small`) on the Amazon Fine Food Reviews dataset to generate product review summaries. It also evaluates \"forgetting\" by checking the model's performance on a set of general knowledge questions before and after fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c474534",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Installing required libraries: `transformers`, `datasets`, `evaluate`, `rouge_score`, `accelerate`, `sentencepiece`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate rouge_score accelerate sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# The code below was generated by AI; see [2].\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d2cd1",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data\n",
    "\n",
    "We use the Amazon Fine Food Reviews dataset. \n",
    "**Important:** You must upload `Reviews.csv` to the Colab runtime files (left sidebar) before running this cell.\n",
    "\n",
    "We will:\n",
    "1. Load the CSV.\n",
    "2. Drop rows with missing values.\n",
    "3. Sample the data (e.g., 20,000 rows) to keep training time reasonable.\n",
    "4. Split into Train (80%), Validation (10%), and Test (10%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556933dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# The code below was generated by AI; see [2].\n",
    "try:\n",
    "    df = pd.read_csv(\"Reviews.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Reviews.csv not found. Please upload it to the Colab runtime.\")\n",
    "    # Create dummy data for demonstration purposes if file is missing so notebook can still 'run' structurally\n",
    "    data = {\n",
    "        \"Summary\": [\"Great product\", \"Not good\", \"Okay item\"] * 100,\n",
    "        \"Text\": [\"This is a really great product I loved it.\", \"This was terrible do not buy.\", \"It was just okay nothing special.\"] * 100\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "# Keep relevant columns and drop NaNs\n",
    "df = df[[\"Summary\", \"Text\"]].dropna()\n",
    "\n",
    "# Filter out very long reviews to save memory/time\n",
    "df = df[df[\"Text\"].str.len() <= 512]\n",
    "\n",
    "# Sample data for faster training (adjust as needed)\n",
    "SAMPLE_SIZE = 20000\n",
    "if len(df) > SAMPLE_SIZE:\n",
    "    df = df.sample(SAMPLE_SIZE, random_state=42)\n",
    "\n",
    "print(f\"Dataset size: {len(df)}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89942d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The code below was generated by AI; see [2].\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_ds = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "test_ds = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e28c502",
   "metadata": {},
   "source": [
    "## 3. Model and Tokenizer Setup\n",
    "\n",
    "We use `google/flan-t5-small`. We load two copies:\n",
    "1. `base_model`: Keeps original weights to measure baseline performance and forgetting.\n",
    "2. `model`: Will be fine-tuned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf2c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Model to be fine-tuned\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Base model for comparison (frozen)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "base_model.to(device)\n",
    "print(\"Models loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b6d2c",
   "metadata": {},
   "source": [
    "## 4. Tokenization\n",
    "\n",
    "We preprocess the text inputs with a prefix \"Summarize this review: \".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ba2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 32\n",
    "PREFIX = \"Summarize this review: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [PREFIX + doc for doc in examples[\"Text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"Summary\"], max_length=MAX_TARGET_LENGTH, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = train_ds.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_ds.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_ds.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df861d9",
   "metadata": {},
   "source": [
    "## 5. Forgetting Analysis (Before Training)\n",
    "\n",
    "We define a small set of general knowledge questions to test the \"forgetting\" hypothesis. We check how well the base model answers them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2964199",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = [\n",
    "    (\"What is the capital of France?\", \"Paris\"),\n",
    "    (\"How many days are in a week?\", \"7\"),\n",
    "    (\"What gas do plants absorb?\", \"carbon dioxide\"),\n",
    "    (\"What is the largest planet in our solar system?\", \"Jupiter\"),\n",
    "    (\"What is H2O?\", \"water\"),\n",
    "    (\"Who wrote Romeo and Juliet?\", \"Shakespeare\"),\n",
    "    (\"What color is the sky on a clear day?\", \"blue\"),\n",
    "    (\"What is 2 + 2?\", \"4\")\n",
    "]\n",
    "\n",
    "def evaluate_forgetting(model_obj, tokenizer_obj, questions, device):\n",
    "    model_obj.eval()\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    print(\"--- Forgetting Analysis ---\")\n",
    "    for q, ans in questions:\n",
    "        input_ids = tokenizer_obj(\"Answer the question: \" + q, return_tensors=\"pt\").input_ids.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model_obj.generate(input_ids, max_length=20)\n",
    "        \n",
    "        pred = tokenizer_obj.decode(outputs[0], skip_special_tokens=True)\n",
    "        is_correct = ans.lower() in pred.lower()\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            \n",
    "        results.append({\"Question\": q, \"Expected\": ans, \"Predicted\": pred, \"Correct\": is_correct})\n",
    "        print(f\"Q: {q} | Pred: {pred} | Expected: {ans}\")\n",
    "    \n",
    "    accuracy = correct / len(questions)\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    return accuracy, results\n",
    "\n",
    "print(\"Evaluating Base Model on QA set...\")\n",
    "base_qa_acc, base_qa_results = evaluate_forgetting(base_model, tokenizer, qa_pairs, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e64b2d0",
   "metadata": {},
   "source": [
    "## 6. Fine-Tuning\n",
    "\n",
    "We use `Seq2SeqTrainer` to fine-tune the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan-t5-summarizer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=(device == \"cuda\"),\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# The code below was generated by AI; see [2].\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b48a58",
   "metadata": {},
   "source": [
    "## 7. Evaluation: Summarization Quality\n",
    "\n",
    "Compare ROUGE scores and look at qualitative examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6187b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on Test Set...\")\n",
    "test_results = trainer.evaluate(tokenized_test)\n",
    "print(test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e43bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative Comparison\n",
    "def generate_summary(model_obj, text, device):\n",
    "    inputs = tokenizer(PREFIX + text, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True).to(device)\n",
    "    outputs = model_obj.generate(inputs.input_ids, max_length=MAX_TARGET_LENGTH, num_beams=4)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "sample_indices = [0, 5, 10, 15, 20]\n",
    "print(\"--- Qualitative Results ---\\n\")\n",
    "\n",
    "for idx in sample_indices:\n",
    "    example = test_ds[idx]\n",
    "    text = example[\"Text\"]\n",
    "    ref_summary = example[\"Summary\"]\n",
    "    \n",
    "    base_summary = generate_summary(base_model, text, device)\n",
    "    ft_summary = generate_summary(model, text, device)\n",
    "    \n",
    "    print(f\"Review: {text[:200]}...\")\n",
    "    print(f\"Reference: {ref_summary}\")\n",
    "    print(f\"Base Model: {base_summary}\")\n",
    "    print(f\"Fine-Tuned: {ft_summary}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39537a45",
   "metadata": {},
   "source": [
    "## 8. Forgetting Analysis (After Training)\n",
    "\n",
    "Check if the fine-tuned model has forgotten general knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73520c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Fine-Tuned Model on QA set...\")\n",
    "ft_qa_acc, ft_qa_results = evaluate_forgetting(model, tokenizer, qa_pairs, device)\n",
    "\n",
    "print(f\"\\nBase Model QA Accuracy: {base_qa_acc:.2%}\")\n",
    "print(f\"Fine-Tuned Model QA Accuracy: {ft_qa_acc:.2%}\")\n",
    "\n",
    "diff = ft_qa_acc - base_qa_acc\n",
    "print(f\"Change in Accuracy: {diff:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5228e4d8",
   "metadata": {},
   "source": [
    "## 9. Save Model\n",
    "\n",
    "Save the fine-tuned model to be downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./finetuned_summarizer_final\")\n",
    "tokenizer.save_pretrained(\"./finetuned_summarizer_final\")\n",
    "\n",
    "print(\"Model saved to ./finetuned_summarizer_final\")\n",
    "# To download from Colab:\n",
    "# from google.colab import files\n",
    "# !zip -r model.zip ./finetuned_summarizer_final\n",
    "# files.download('model.zip')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
