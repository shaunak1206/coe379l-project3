{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659f2a7a",
   "metadata": {},
   "source": [
    "# Project 3: Fine-Tuning FLAN-T5 for Summarization & Measuring Forgetting\n",
    "\n",
    "**Authors:** Shaunak Kapur & Pranav Krishnan\n",
    "\n",
    "This notebook implements the Project 3 proposal: fine-tuning a small language model (`google/flan-t5-small`) on the Amazon Fine Food Reviews dataset to generate product review summaries. It also evaluates \"forgetting\" by checking the model's performance on a set of general knowledge questions before and after fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50a91d",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Installing required libraries: `transformers`, `datasets`, `evaluate`, `rouge_score`, `accelerate`, `sentencepiece`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d22324",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate rouge_score accelerate sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1e698",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data\n",
    "\n",
    "We use the Amazon Fine Food Reviews dataset from Hugging Face. The dataset will be automatically downloaded using `load_dataset`.\n",
    "\n",
    "We will:\n",
    "1. Download the dataset from Hugging Face.\n",
    "2. Convert to pandas DataFrame.\n",
    "3. Drop rows with missing values.\n",
    "4. Sample the data (e.g., 20,000 rows) to keep training time reasonable.\n",
    "5. Split into Train (80%), Validation (10%), and Test (10%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db864825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: Downloading dataset from Hugging Face...\")\n",
    "print(\"=\" * 80)\n",
    "ds = load_dataset(\"jhan21/amazon-food-reviews-dataset\")\n",
    "print(f\"✓ Dataset loaded. Available splits: {list(ds.keys())}\")\n",
    "\n",
    "# Convert to pandas DataFrame (the dataset has a 'train' split)\n",
    "print(\"\\nConverting to pandas DataFrame...\")\n",
    "df = ds[\"train\"].to_pandas()\n",
    "print(f\"✓ Original dataset size: {len(df)} rows, {len(df.columns)} columns\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "\n",
    "# Keep relevant columns and drop NaNs\n",
    "print(\"\\nFiltering data...\")\n",
    "print(f\"  Before filtering: {len(df)} rows\")\n",
    "df = df[[\"Summary\", \"Text\"]].dropna()\n",
    "print(f\"  After dropping NaN: {len(df)} rows\")\n",
    "\n",
    "# Filter out very long reviews to save memory/time\n",
    "df = df[df[\"Text\"].str.len() <= 512]\n",
    "print(f\"  After filtering long reviews (<=512 chars): {len(df)} rows\")\n",
    "\n",
    "# Sample data for faster training (adjust as needed)\n",
    "SAMPLE_SIZE = 20000\n",
    "if len(df) > SAMPLE_SIZE:\n",
    "    print(f\"\\nSampling {SAMPLE_SIZE} rows from {len(df)} total rows...\")\n",
    "    df = df.sample(SAMPLE_SIZE, random_state=42)\n",
    "    print(f\"✓ Sampled dataset size: {len(df)} rows\")\n",
    "else:\n",
    "    print(f\"\\nUsing full dataset: {len(df)} rows\")\n",
    "\n",
    "print(\"\\nSample data preview:\")\n",
    "print(df.head(3))\n",
    "print(f\"\\n✓ Dataset ready: {len(df)} rows\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 2: Splitting dataset into train/val/test...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"✓ Split complete:\")\n",
    "print(f\"  Train: {len(train_df)} rows (80%)\")\n",
    "print(f\"  Validation: {len(val_df)} rows (10%)\")\n",
    "print(f\"  Test: {len(test_df)} rows (10%)\")\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_ds = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "test_ds = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "print(f\"\\n✓ Datasets created:\")\n",
    "print(f\"  Train: {len(train_ds)} samples\")\n",
    "print(f\"  Val: {len(val_ds)} samples\")\n",
    "print(f\"  Test: {len(test_ds)} samples\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b02077",
   "metadata": {},
   "source": [
    "## 3. Model and Tokenizer Setup\n",
    "\n",
    "We use `google/flan-t5-small`. We load two copies:\n",
    "1. `base_model`: Keeps original weights to measure baseline performance and forgetting.\n",
    "2. `model`: Will be fine-tuned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5434b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection based on AI recommendation; see [1]\n",
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 3: Loading model and tokenizer...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(\"\\nLoading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"✓ Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"  Pad token: {tokenizer.pad_token_id}, EOS token: {tokenizer.eos_token_id}\")\n",
    "\n",
    "print(\"\\nLoading model for fine-tuning...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "print(f\"✓ Model loaded. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(\"\\nLoading base model for comparison...\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "base_model.to(device)\n",
    "print(f\"✓ Base model loaded and moved to {device}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec12bb09",
   "metadata": {},
   "source": [
    "## 4. Tokenization\n",
    "\n",
    "We preprocess the text inputs with a prefix \"Summarize this review: \".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 32\n",
    "PREFIX = \"Summarize this review: \"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 4: Tokenizing datasets...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Max input length: {MAX_INPUT_LENGTH} tokens\")\n",
    "print(f\"Max target length: {MAX_TARGET_LENGTH} tokens\")\n",
    "print(f\"Prefix: '{PREFIX}'\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [PREFIX + doc for doc in examples[\"Text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"Summary\"], max_length=MAX_TARGET_LENGTH, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "print(\"\\nTokenizing training set...\")\n",
    "tokenized_train = train_ds.map(preprocess_function, batched=True)\n",
    "print(f\"✓ Train tokenized: {len(tokenized_train)} samples\")\n",
    "\n",
    "print(\"Tokenizing validation set...\")\n",
    "tokenized_val = val_ds.map(preprocess_function, batched=True)\n",
    "print(f\"✓ Val tokenized: {len(tokenized_val)} samples\")\n",
    "\n",
    "print(\"Tokenizing test set...\")\n",
    "tokenized_test = test_ds.map(preprocess_function, batched=True)\n",
    "print(f\"✓ Test tokenized: {len(tokenized_test)} samples\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\nExample tokenized input:\")\n",
    "example = tokenized_train[0]\n",
    "print(f\"  Input IDs length: {len(example['input_ids'])}\")\n",
    "print(f\"  Labels length: {len(example['labels'])}\")\n",
    "print(f\"  Decoded input: {tokenizer.decode(example['input_ids'][:50])}...\")\n",
    "print(f\"  Decoded label: {tokenizer.decode([l for l in example['labels'] if l != -100])}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ff7b90",
   "metadata": {},
   "source": [
    "## 5. Forgetting Analysis (Before Training)\n",
    "\n",
    "We define a small set of general knowledge questions to test the \"forgetting\" hypothesis. We check how well the base model answers them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa42108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forgetting analysis approach based on AI recommendation; see [4]\n",
    "qa_pairs = [\n",
    "    (\"What is the capital of France?\", \"Paris\"),\n",
    "    (\"How many days are in a week?\", \"7\"),\n",
    "    (\"What gas do plants absorb?\", \"carbon dioxide\"),\n",
    "    (\"What is the largest planet in our solar system?\", \"Jupiter\"),\n",
    "    (\"What is H2O?\", \"water\"),\n",
    "    (\"Who wrote Romeo and Juliet?\", \"Shakespeare\"),\n",
    "    (\"What color is the sky on a clear day?\", \"blue\"),\n",
    "    (\"What is 2 + 2?\", \"4\")\n",
    "]\n",
    "\n",
    "def evaluate_forgetting(model_obj, tokenizer_obj, questions, device):\n",
    "    model_obj.eval()\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    print(\"--- Forgetting Analysis ---\")\n",
    "    for q, ans in questions:\n",
    "        # FLAN-T5 prompt format based on AI guidance; see [5]\n",
    "        prompt = f\"Question: {q}\\nAnswer:\"\n",
    "        input_ids = tokenizer_obj(prompt, return_tensors=\"pt\", max_length=128, truncation=True).input_ids.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model_obj.generate(\n",
    "                input_ids, \n",
    "                max_length=50,\n",
    "                num_beams=2,\n",
    "                early_stopping=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        pred = tokenizer_obj.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        \n",
    "        # More flexible answer matching\n",
    "        pred_lower = pred.lower()\n",
    "        ans_lower = ans.lower()\n",
    "        \n",
    "        # Check if answer is in prediction (handles partial matches)\n",
    "        is_correct = (\n",
    "            ans_lower in pred_lower or \n",
    "            pred_lower in ans_lower or\n",
    "            any(word in pred_lower for word in ans_lower.split() if len(word) > 2)\n",
    "        )\n",
    "        \n",
    "        # Special cases for numeric answers\n",
    "        if ans.isdigit():\n",
    "            # Extract numbers from prediction\n",
    "            import re\n",
    "            numbers = re.findall(r'\\d+', pred)\n",
    "            is_correct = ans in numbers or is_correct\n",
    "        \n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            \n",
    "        results.append({\"Question\": q, \"Expected\": ans, \"Predicted\": pred, \"Correct\": is_correct})\n",
    "        print(f\"Q: {q}\")\n",
    "        print(f\"  Expected: {ans} | Predicted: {pred} | {'✓' if is_correct else '✗'}\")\n",
    "    \n",
    "    accuracy = correct / len(questions)\n",
    "    print(f\"\\nAccuracy: {accuracy:.2%} ({correct}/{len(questions)})\")\n",
    "    return accuracy, results\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 5: Evaluating Base Model on QA set (Before Training)...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Number of QA pairs: {len(qa_pairs)}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Base model on device: {next(base_model.parameters()).device}\")\n",
    "\n",
    "try:\n",
    "    base_qa_acc, base_qa_results = evaluate_forgetting(base_model, tokenizer, qa_pairs, device)\n",
    "    print(f\"\\n✓ Base model evaluation complete!\")\n",
    "    print(\"=\" * 80)\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Base model evaluation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c7c6d",
   "metadata": {},
   "source": [
    "## 6. Fine-Tuning\n",
    "\n",
    "We use `Seq2SeqTrainer` to fine-tune the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE metric implementation based on AI guidance; see [2]\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "print(\"✓ ROUGE metric loaded\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        print(f\"\\n[DEBUG] compute_metrics called - predictions shape: {np.array(predictions).shape}, labels shape: {np.array(labels).shape}\")\n",
    "        \n",
    "        # Convert to numpy if needed and ensure valid token IDs\n",
    "        predictions = np.array(predictions)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Clip predictions to valid token ID range (0 to vocab_size-1) - fix for OverflowError; see [3]\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        predictions = np.clip(predictions, 0, vocab_size - 1)\n",
    "        print(f\"[DEBUG] Predictions clipped to vocab range [0, {vocab_size-1}]\")\n",
    "        \n",
    "        # Decode predictions\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        print(f\"[DEBUG] Decoded {len(decoded_preds)} predictions\")\n",
    "        \n",
    "        # Replace -100 (ignored labels) with pad_token_id for decoding\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        labels = np.clip(labels, 0, vocab_size - 1)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        print(f\"[DEBUG] Decoded {len(decoded_labels)} labels\")\n",
    "        \n",
    "        # Compute ROUGE\n",
    "        result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        print(f\"[DEBUG] ROUGE computed: {result}\")\n",
    "        \n",
    "        # Calculate actual generation length (only count non-padding tokens up to EOS)\n",
    "        gen_lens = []\n",
    "        for pred in predictions:\n",
    "            # Find EOS token or count non-padding tokens\n",
    "            pred_list = pred.tolist() if hasattr(pred, 'tolist') else list(pred)\n",
    "            # Remove padding tokens (0) and count until EOS (1 for T5)\n",
    "            length = 0\n",
    "            for token_id in pred_list:\n",
    "                if token_id == tokenizer.eos_token_id or token_id == 1:  # EOS token\n",
    "                    break\n",
    "                if token_id != tokenizer.pad_token_id and token_id != 0:\n",
    "                    length += 1\n",
    "            gen_lens.append(length)\n",
    "        \n",
    "        avg_gen_len = np.mean(gen_lens) if gen_lens else 0\n",
    "        result[\"gen_len\"] = avg_gen_len\n",
    "        print(f\"[DEBUG] Average generation length: {avg_gen_len:.2f}\")\n",
    "        \n",
    "        # Convert ROUGE scores to percentages (but NOT gen_len)\n",
    "        final_result = {}\n",
    "        for k, v in result.items():\n",
    "            if k == \"gen_len\":\n",
    "                final_result[k] = round(v, 2)  # Keep gen_len as-is, just round\n",
    "            else:\n",
    "                final_result[k] = round(v * 100, 4)  # Convert ROUGE to percentage\n",
    "        \n",
    "        print(f\"[DEBUG] Final metrics: {final_result}\")\n",
    "        return final_result\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] compute_metrics failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0, \"gen_len\": 0.0}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 6: Setting up training...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan-t5-summarizer\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=(device == \"cuda\"),\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {args.learning_rate}\")\n",
    "print(f\"  FP16: {args.fp16}\")\n",
    "print(f\"  Output dir: {args.output_dir}\")\n",
    "\n",
    "print(\"\\nCreating trainer...\")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(f\"✓ Trainer created\")\n",
    "print(f\"  Train samples: {len(tokenized_train)}\")\n",
    "print(f\"  Eval samples: {len(tokenized_val)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"✓ Training completed successfully!\")\n",
    "    print(\"=\" * 80)\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Training failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36373d51",
   "metadata": {},
   "source": [
    "## 7. Evaluation: Summarization Quality\n",
    "\n",
    "Compare ROUGE scores and look at qualitative examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STEP 7: Evaluating on test set...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Test samples: {len(tokenized_test)}\")\n",
    "\n",
    "try:\n",
    "    test_results = trainer.evaluate(tokenized_test)\n",
    "    print(\"\\n✓ Test evaluation complete!\")\n",
    "    print(\"\\nTest Results:\")\n",
    "    for key, value in test_results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    print(\"=\" * 80)\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Test evaluation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative Comparison\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 8: Qualitative Comparison...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def generate_summary(model_obj, text, device):\n",
    "    try:\n",
    "        inputs = tokenizer(PREFIX + text, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True).to(device)\n",
    "        outputs = model_obj.generate(inputs.input_ids, max_length=MAX_TARGET_LENGTH, num_beams=4)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] generate_summary failed: {e}\")\n",
    "        return f\"[ERROR: {str(e)}]\"\n",
    "\n",
    "sample_indices = [0, 5, 10, 15, 20]\n",
    "print(f\"Comparing {len(sample_indices)} examples from test set...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    try:\n",
    "        print(f\"\\nExample {i}/{len(sample_indices)} (Index {idx}):\")\n",
    "        example = test_ds[idx]\n",
    "        text = example[\"Text\"]\n",
    "        ref_summary = example[\"Summary\"]\n",
    "        \n",
    "        print(f\"  Generating base model summary...\")\n",
    "        base_summary = generate_summary(base_model, text, device)\n",
    "        \n",
    "        print(f\"  Generating fine-tuned model summary...\")\n",
    "        ft_summary = generate_summary(model, text, device)\n",
    "        \n",
    "        print(f\"\\n  Review: {text[:200]}...\")\n",
    "        print(f\"  Reference: {ref_summary}\")\n",
    "        print(f\"  Base Model: {base_summary}\")\n",
    "        print(f\"  Fine-Tuned: {ft_summary}\")\n",
    "        print(\"-\" * 80)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process example {idx}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n✓ Qualitative comparison complete!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3407042",
   "metadata": {},
   "source": [
    "## 8. Forgetting Analysis (After Training)\n",
    "\n",
    "Check if the fine-tuned model has forgotten general knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36285762",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STEP 9: Evaluating Fine-Tuned Model on QA set (After Training)...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Number of QA pairs: {len(qa_pairs)}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Fine-tuned model on device: {next(model.parameters()).device}\")\n",
    "\n",
    "try:\n",
    "    ft_qa_acc, ft_qa_results = evaluate_forgetting(model, tokenizer, qa_pairs, device)\n",
    "    print(f\"\\n✓ Fine-tuned model evaluation complete!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FORGETTING ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Base Model QA Accuracy: {base_qa_acc:.2%} ({base_qa_acc * len(qa_pairs):.0f}/{len(qa_pairs)})\")\n",
    "    print(f\"Fine-Tuned Model QA Accuracy: {ft_qa_acc:.2%} ({ft_qa_acc * len(qa_pairs):.0f}/{len(qa_pairs)})\")\n",
    "    \n",
    "    diff = ft_qa_acc - base_qa_acc\n",
    "    print(f\"Change in Accuracy: {diff:+.2%}\")\n",
    "    \n",
    "    if diff < 0:\n",
    "        print(f\"⚠️  Forgetting detected! Model lost {abs(diff):.2%} accuracy on general knowledge.\")\n",
    "    elif diff > 0:\n",
    "        print(f\"✓ Model improved by {diff:.2%} (unexpected but good!)\")\n",
    "    else:\n",
    "        print(f\"→ No change in general knowledge performance.\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Fine-tuned model evaluation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd78ec",
   "metadata": {},
   "source": [
    "## 9. Save Model\n",
    "\n",
    "Save the fine-tuned model to be downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0503d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./finetuned_summarizer_final\")\n",
    "tokenizer.save_pretrained(\"./finetuned_summarizer_final\")\n",
    "\n",
    "print(\"Model saved to ./finetuned_summarizer_final\")\n",
    "# To download from Colab:\n",
    "# from google.colab import files\n",
    "# !zip -r model.zip ./finetuned_summarizer_final\n",
    "# files.download('model.zip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f3bfb3",
   "metadata": {},
   "source": [
    "# Project 3: Fine-Tuning FLAN-T5 for Summarization & Measuring Forgetting\n",
    "\n",
    "**Authors:** Shaunak Kapur & Pranav Krishnan\n",
    "\n",
    "This notebook implements the Project 3 proposal: fine-tuning a small language model (`google/flan-t5-small`) on the Amazon Fine Food Reviews dataset to generate product review summaries. It also evaluates \"forgetting\" by checking the model's performance on a set of general knowledge questions before and after fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c474534",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Installing required libraries: `transformers`, `datasets`, `evaluate`, `rouge_score`, `accelerate`, `sentencepiece`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate rouge_score accelerate sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# The code below was generated by AI; see [2].\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d2cd1",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data\n",
    "\n",
    "We use the Amazon Fine Food Reviews dataset. \n",
    "**Important:** You must upload `Reviews.csv` to the Colab runtime files (left sidebar) before running this cell.\n",
    "\n",
    "We will:\n",
    "1. Load the CSV.\n",
    "2. Drop rows with missing values.\n",
    "3. Sample the data (e.g., 20,000 rows) to keep training time reasonable.\n",
    "4. Split into Train (80%), Validation (10%), and Test (10%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556933dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# The code below was generated by AI; see [2].\n",
    "try:\n",
    "    df = pd.read_csv(\"Reviews.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Reviews.csv not found. Please upload it to the Colab runtime.\")\n",
    "    # Create dummy data for demonstration purposes if file is missing so notebook can still 'run' structurally\n",
    "    data = {\n",
    "        \"Summary\": [\"Great product\", \"Not good\", \"Okay item\"] * 100,\n",
    "        \"Text\": [\"This is a really great product I loved it.\", \"This was terrible do not buy.\", \"It was just okay nothing special.\"] * 100\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "# Keep relevant columns and drop NaNs\n",
    "df = df[[\"Summary\", \"Text\"]].dropna()\n",
    "\n",
    "# Filter out very long reviews to save memory/time\n",
    "df = df[df[\"Text\"].str.len() <= 512]\n",
    "\n",
    "# Sample data for faster training (adjust as needed)\n",
    "SAMPLE_SIZE = 20000\n",
    "if len(df) > SAMPLE_SIZE:\n",
    "    df = df.sample(SAMPLE_SIZE, random_state=42)\n",
    "\n",
    "print(f\"Dataset size: {len(df)}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89942d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The code below was generated by AI; see [2].\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_ds = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "test_ds = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e28c502",
   "metadata": {},
   "source": [
    "## 3. Model and Tokenizer Setup\n",
    "\n",
    "We use `google/flan-t5-small`. We load two copies:\n",
    "1. `base_model`: Keeps original weights to measure baseline performance and forgetting.\n",
    "2. `model`: Will be fine-tuned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf2c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Model to be fine-tuned\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Base model for comparison (frozen)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "base_model.to(device)\n",
    "print(\"Models loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b6d2c",
   "metadata": {},
   "source": [
    "## 4. Tokenization\n",
    "\n",
    "We preprocess the text inputs with a prefix \"Summarize this review: \".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ba2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 32\n",
    "PREFIX = \"Summarize this review: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [PREFIX + doc for doc in examples[\"Text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"Summary\"], max_length=MAX_TARGET_LENGTH, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = train_ds.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_ds.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_ds.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df861d9",
   "metadata": {},
   "source": [
    "## 5. Forgetting Analysis (Before Training)\n",
    "\n",
    "We define a small set of general knowledge questions to test the \"forgetting\" hypothesis. We check how well the base model answers them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2964199",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = [\n",
    "    (\"What is the capital of France?\", \"Paris\"),\n",
    "    (\"How many days are in a week?\", \"7\"),\n",
    "    (\"What gas do plants absorb?\", \"carbon dioxide\"),\n",
    "    (\"What is the largest planet in our solar system?\", \"Jupiter\"),\n",
    "    (\"What is H2O?\", \"water\"),\n",
    "    (\"Who wrote Romeo and Juliet?\", \"Shakespeare\"),\n",
    "    (\"What color is the sky on a clear day?\", \"blue\"),\n",
    "    (\"What is 2 + 2?\", \"4\")\n",
    "]\n",
    "\n",
    "def evaluate_forgetting(model_obj, tokenizer_obj, questions, device):\n",
    "    model_obj.eval()\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    print(\"--- Forgetting Analysis ---\")\n",
    "    for q, ans in questions:\n",
    "        input_ids = tokenizer_obj(\"Answer the question: \" + q, return_tensors=\"pt\").input_ids.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model_obj.generate(input_ids, max_length=20)\n",
    "        \n",
    "        pred = tokenizer_obj.decode(outputs[0], skip_special_tokens=True)\n",
    "        is_correct = ans.lower() in pred.lower()\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            \n",
    "        results.append({\"Question\": q, \"Expected\": ans, \"Predicted\": pred, \"Correct\": is_correct})\n",
    "        print(f\"Q: {q} | Pred: {pred} | Expected: {ans}\")\n",
    "    \n",
    "    accuracy = correct / len(questions)\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    return accuracy, results\n",
    "\n",
    "print(\"Evaluating Base Model on QA set...\")\n",
    "base_qa_acc, base_qa_results = evaluate_forgetting(base_model, tokenizer, qa_pairs, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e64b2d0",
   "metadata": {},
   "source": [
    "## 6. Fine-Tuning\n",
    "\n",
    "We use `Seq2SeqTrainer` to fine-tune the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan-t5-summarizer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=(device == \"cuda\"),\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# The code below was generated by AI; see [2].\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b48a58",
   "metadata": {},
   "source": [
    "## 7. Evaluation: Summarization Quality\n",
    "\n",
    "Compare ROUGE scores and look at qualitative examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6187b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on Test Set...\")\n",
    "test_results = trainer.evaluate(tokenized_test)\n",
    "print(test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e43bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative Comparison\n",
    "def generate_summary(model_obj, text, device):\n",
    "    inputs = tokenizer(PREFIX + text, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True).to(device)\n",
    "    outputs = model_obj.generate(inputs.input_ids, max_length=MAX_TARGET_LENGTH, num_beams=4)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "sample_indices = [0, 5, 10, 15, 20]\n",
    "print(\"--- Qualitative Results ---\\n\")\n",
    "\n",
    "for idx in sample_indices:\n",
    "    example = test_ds[idx]\n",
    "    text = example[\"Text\"]\n",
    "    ref_summary = example[\"Summary\"]\n",
    "    \n",
    "    base_summary = generate_summary(base_model, text, device)\n",
    "    ft_summary = generate_summary(model, text, device)\n",
    "    \n",
    "    print(f\"Review: {text[:200]}...\")\n",
    "    print(f\"Reference: {ref_summary}\")\n",
    "    print(f\"Base Model: {base_summary}\")\n",
    "    print(f\"Fine-Tuned: {ft_summary}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39537a45",
   "metadata": {},
   "source": [
    "## 8. Forgetting Analysis (After Training)\n",
    "\n",
    "Check if the fine-tuned model has forgotten general knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73520c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Fine-Tuned Model on QA set...\")\n",
    "ft_qa_acc, ft_qa_results = evaluate_forgetting(model, tokenizer, qa_pairs, device)\n",
    "\n",
    "print(f\"\\nBase Model QA Accuracy: {base_qa_acc:.2%}\")\n",
    "print(f\"Fine-Tuned Model QA Accuracy: {ft_qa_acc:.2%}\")\n",
    "\n",
    "diff = ft_qa_acc - base_qa_acc\n",
    "print(f\"Change in Accuracy: {diff:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5228e4d8",
   "metadata": {},
   "source": [
    "## 9. Save Model\n",
    "\n",
    "Save the fine-tuned model to be downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./finetuned_summarizer_final\")\n",
    "tokenizer.save_pretrained(\"./finetuned_summarizer_final\")\n",
    "\n",
    "print(\"Model saved to ./finetuned_summarizer_final\")\n",
    "# To download from Colab:\n",
    "# from google.colab import files\n",
    "# !zip -r model.zip ./finetuned_summarizer_final\n",
    "# files.download('model.zip')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
