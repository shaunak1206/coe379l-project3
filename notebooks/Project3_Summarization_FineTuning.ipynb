{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "659f2a7a",
   "metadata": {},
   "source": [
    "# Project 3: Fine-Tuning FLAN-T5 for Summarization & Measuring Forgetting\n",
    "\n",
    "**Authors:** Shaunak Kapur & Pranav Krishnan\n",
    "\n",
    "This notebook implements the Project 3 proposal: fine-tuning a small language model (`google/flan-t5-small`) on the Amazon Fine Food Reviews dataset to generate product review summaries. It also evaluates \"forgetting\" by checking the model's performance on a set of general knowledge questions before and after fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50a91d",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Installing required libraries: `transformers`, `datasets`, `evaluate`, `rouge_score`, `accelerate`, `sentencepiece`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d22324",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate rouge_score accelerate sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd8fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1e698",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data\n",
    "\n",
    "We use the Amazon Fine Food Reviews dataset from Hugging Face. The dataset will be automatically downloaded using `load_dataset`.\n",
    "\n",
    "We will:\n",
    "1. Download the dataset from Hugging Face.\n",
    "2. Convert to pandas DataFrame.\n",
    "3. Drop rows with missing values.\n",
    "4. Sample the data (e.g., 20,000 rows) to keep training time reasonable.\n",
    "5. Split into Train (80%), Validation (10%), and Test (10%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db864825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: Downloading dataset from Hugging Face...\")\n",
    "print(\"=\" * 80)\n",
    "ds = load_dataset(\"jhan21/amazon-food-reviews-dataset\")\n",
    "print(f\"\u2713 Dataset loaded. Available splits: {list(ds.keys())}\")\n",
    "\n",
    "# Convert to pandas DataFrame (the dataset has a 'train' split)\n",
    "print(\"\\nConverting to pandas DataFrame...\")\n",
    "df = ds[\"train\"].to_pandas()\n",
    "print(f\"\u2713 Original dataset size: {len(df)} rows, {len(df.columns)} columns\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "\n",
    "# Keep relevant columns and drop NaNs\n",
    "print(\"\\nFiltering data...\")\n",
    "print(f\"  Before filtering: {len(df)} rows\")\n",
    "df = df[[\"Summary\", \"Text\"]].dropna()\n",
    "print(f\"  After dropping NaN: {len(df)} rows\")\n",
    "\n",
    "# Filter out very long reviews to save memory/time\n",
    "df = df[df[\"Text\"].str.len() <= 512]\n",
    "print(f\"  After filtering long reviews (<=512 chars): {len(df)} rows\")\n",
    "\n",
    "# Sample data for faster training (adjust as needed)\n",
    "SAMPLE_SIZE = 20000\n",
    "if len(df) > SAMPLE_SIZE:\n",
    "    print(f\"\\nSampling {SAMPLE_SIZE} rows from {len(df)} total rows...\")\n",
    "    df = df.sample(SAMPLE_SIZE, random_state=42)\n",
    "    print(f\"\u2713 Sampled dataset size: {len(df)} rows\")\n",
    "else:\n",
    "    print(f\"\\nUsing full dataset: {len(df)} rows\")\n",
    "\n",
    "print(\"\\nSample data preview:\")\n",
    "print(df.head(3))\n",
    "print(f\"\\n\u2713 Dataset ready: {len(df)} rows\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 2: Splitting dataset into train/val/test...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"\u2713 Split complete:\")\n",
    "print(f\"  Train: {len(train_df)} rows (80%)\")\n",
    "print(f\"  Validation: {len(val_df)} rows (10%)\")\n",
    "print(f\"  Test: {len(test_df)} rows (10%)\")\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_ds = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "test_ds = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "print(f\"\\n\u2713 Datasets created:\")\n",
    "print(f\"  Train: {len(train_ds)} samples\")\n",
    "print(f\"  Val: {len(val_ds)} samples\")\n",
    "print(f\"  Test: {len(test_ds)} samples\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b02077",
   "metadata": {},
   "source": [
    "## 3. Model and Tokenizer Setup\n",
    "\n",
    "We use `google/flan-t5-small`. We load two copies:\n",
    "1. `base_model`: Keeps original weights to measure baseline performance and forgetting.\n",
    "2. `model`: Will be fine-tuned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5434b2",
   "metadata": {},
   "outputs": [],
   "source": "# Model selection based on AI recommendation; see [1]\nMODEL_NAME = \"google/flan-t5-small\"\n\nprint(\"=\" * 80)\nprint(\"STEP 3: Loading model and tokenizer...\")\nprint(\"=\" * 80)\nprint(f\"Model: {MODEL_NAME}\")\nprint(f\"Device: {device}\")\n\nprint(\"\\nLoading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nprint(f\"\u2713 Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")\nprint(f\"  Pad token: {tokenizer.pad_token_id}, EOS token: {tokenizer.eos_token_id}\")\n\nprint(\"\\nLoading model for fine-tuning (GPU-optimized)...\")\n# GPU-OPTIMIZED: Load model directly on GPU with explicit dtype\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float32,  # Explicit FP32 dtype\n    device_map=\"auto\"  # Auto-place model on GPU (no CPU intermediate step)\n)\nmodel.train()  # Explicit training mode\nprint(f\"\u2713 Model loaded. Parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"  Model device: {next(model.parameters()).device}\")\nprint(f\"  Model dtype: {next(model.parameters()).dtype}\")\n\nprint(\"\\nLoading base model for comparison...\")\nbase_model = AutoModelForSeq2SeqLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float32,\n    device_map=\"auto\"\n)\nbase_model.eval()  # Set to eval mode (won't be trained)\nprint(f\"\u2713 Base model loaded\")\nprint(f\"  Base model device: {next(base_model.parameters()).device}\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "id": "ec12bb09",
   "metadata": {},
   "source": [
    "## 4. Tokenization\n",
    "\n",
    "We preprocess the text inputs with a prefix \"Summarize this review: \".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e46b7",
   "metadata": {},
   "outputs": [],
   "source": "MAX_INPUT_LENGTH = 256\nMAX_TARGET_LENGTH = 32\nPREFIX = \"Summarize this review: \"\n\nprint(\"=\" * 80)\nprint(\"STEP 4: Tokenizing datasets...\")\nprint(\"=\" * 80)\nprint(f\"Max input length: {MAX_INPUT_LENGTH} tokens\")\nprint(f\"Max target length: {MAX_TARGET_LENGTH} tokens\")\nprint(f\"Prefix: '{PREFIX}'\")\n\ndef preprocess_function(examples):\n    inputs = [PREFIX + doc for doc in examples[\"Text\"]]\n    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n\n    labels = tokenizer(text_target=examples[\"Summary\"], max_length=MAX_TARGET_LENGTH, truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\nprint(\"\\nTokenizing training set...\")\ntokenized_train = train_ds.map(preprocess_function, batched=True, remove_columns=train_ds.column_names)\nprint(f\"\u2713 Train tokenized: {len(tokenized_train)} samples\")\nprint(f\"  Columns after tokenization: {tokenized_train.column_names}\")\n\nprint(\"Tokenizing validation set...\")\ntokenized_val = val_ds.map(preprocess_function, batched=True, remove_columns=val_ds.column_names)\nprint(f\"\u2713 Val tokenized: {len(tokenized_val)} samples\")\n\nprint(\"Tokenizing test set...\")\ntokenized_test = test_ds.map(preprocess_function, batched=True, remove_columns=test_ds.column_names)\nprint(f\"\u2713 Test tokenized: {len(tokenized_test)} samples\")\n\n# Show example\nprint(\"\\nExample tokenized input:\")\nexample = tokenized_train[0]\nprint(f\"  Input IDs length: {len(example['input_ids'])}\")\nprint(f\"  Labels length: {len(example['labels'])}\")\nprint(f\"  Decoded input: {tokenizer.decode(example['input_ids'][:50])}...\")\nprint(f\"  Decoded label: {tokenizer.decode([l for l in example['labels'] if l != -100])}\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "id": "53ff7b90",
   "metadata": {},
   "source": [
    "## 5. Forgetting Analysis (Before Training)\n",
    "\n",
    "We define a small set of general knowledge questions to test the \"forgetting\" hypothesis. We check how well the base model answers them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa42108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forgetting analysis approach based on AI recommendation; see [4]\n",
    "qa_pairs = [\n",
    "    (\"What is the capital of France?\", \"Paris\"),\n",
    "    (\"How many days are in a week?\", \"7\"),\n",
    "    (\"What gas do plants absorb?\", \"carbon dioxide\"),\n",
    "    (\"What is the largest planet in our solar system?\", \"Jupiter\"),\n",
    "    (\"What is H2O?\", \"water\"),\n",
    "    (\"Who wrote Romeo and Juliet?\", \"Shakespeare\"),\n",
    "    (\"What color is the sky on a clear day?\", \"blue\"),\n",
    "    (\"What is 2 + 2?\", \"4\")\n",
    "]\n",
    "\n",
    "def evaluate_forgetting(model_obj, tokenizer_obj, questions, device):\n",
    "    model_obj.eval()\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    print(\"--- Forgetting Analysis ---\")\n",
    "    for q, ans in questions:\n",
    "        # FLAN-T5 prompt format based on AI guidance; see [5]\n",
    "        prompt = f\"Question: {q}\\nAnswer:\"\n",
    "        input_ids = tokenizer_obj(prompt, return_tensors=\"pt\", max_length=128, truncation=True).input_ids.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model_obj.generate(\n",
    "                input_ids, \n",
    "                max_length=50,\n",
    "                num_beams=2,\n",
    "                early_stopping=True,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        pred = tokenizer_obj.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        \n",
    "        # More flexible answer matching\n",
    "        pred_lower = pred.lower()\n",
    "        ans_lower = ans.lower()\n",
    "        \n",
    "        # Check if answer is in prediction (handles partial matches)\n",
    "        is_correct = (\n",
    "            ans_lower in pred_lower or \n",
    "            pred_lower in ans_lower or\n",
    "            any(word in pred_lower for word in ans_lower.split() if len(word) > 2)\n",
    "        )\n",
    "        \n",
    "        # Special cases for numeric answers\n",
    "        if ans.isdigit():\n",
    "            # Extract numbers from prediction\n",
    "            import re\n",
    "            numbers = re.findall(r'\\d+', pred)\n",
    "            is_correct = ans in numbers or is_correct\n",
    "        \n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            \n",
    "        results.append({\"Question\": q, \"Expected\": ans, \"Predicted\": pred, \"Correct\": is_correct})\n",
    "        print(f\"Q: {q}\")\n",
    "        print(f\"  Expected: {ans} | Predicted: {pred} | {'\u2713' if is_correct else '\u2717'}\")\n",
    "    \n",
    "    accuracy = correct / len(questions)\n",
    "    print(f\"\\nAccuracy: {accuracy:.2%} ({correct}/{len(questions)})\")\n",
    "    return accuracy, results\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 5: Evaluating Base Model on QA set (Before Training)...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Number of QA pairs: {len(qa_pairs)}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Base model on device: {next(base_model.parameters()).device}\")\n",
    "\n",
    "try:\n",
    "    base_qa_acc, base_qa_results = evaluate_forgetting(base_model, tokenizer, qa_pairs, device)\n",
    "    print(f\"\\n\u2713 Base model evaluation complete!\")\n",
    "    print(\"=\" * 80)\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Base model evaluation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c7c6d",
   "metadata": {},
   "source": [
    "## 6. Fine-Tuning\n",
    "\n",
    "We use `Seq2SeqTrainer` to fine-tune the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf71f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE metric implementation based on AI guidance; see [2]",
    "rouge = evaluate.load(\"rouge\")",
    "print(\"\u2713 ROUGE metric loaded\")",
    "",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")",
    "print(f\"CUDA version: {torch.version.cuda}\")",
    "print(f\"PyTorch version: {torch.__version__}\")",
    "if torch.cuda.is_available():",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")",
    "    print(f\"GPU capability: {torch.cuda.get_device_capability(0)}\")",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")",
    "    print(f\"Supports BF16: {torch.cuda.get_device_capability()[0] >= 7}\")",
    "print(\"=\" * 80)",
    "",
    "",
    "def compute_metrics(eval_pred):",
    "    try:",
    "        predictions, labels = eval_pred        ",
    "        # Convert to numpy if needed and ensure valid token IDs",
    "        predictions = np.array(predictions)",
    "        labels = np.array(labels)",
    "        ",
    "        # Clip predictions to valid token ID range (0 to vocab_size-1) - fix for OverflowError; see [3]",
    "        vocab_size = tokenizer.vocab_size",
    "        predictions = np.clip(predictions, 0, vocab_size - 1)        ",
    "        # Decode predictions",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)        ",
    "        # Replace -100 (ignored labels) with pad_token_id for decoding",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)",
    "        labels = np.clip(labels, 0, vocab_size - 1)",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)        ",
    "        # Compute ROUGE",
    "        result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)        ",
    "        # Calculate actual generation length (only count non-padding tokens up to EOS)",
    "        gen_lens = []",
    "        for pred in predictions:",
    "            # Find EOS token or count non-padding tokens",
    "            pred_list = pred.tolist() if hasattr(pred, 'tolist') else list(pred)",
    "            # Remove padding tokens (0) and count until EOS (1 for T5)",
    "            length = 0",
    "            for token_id in pred_list:",
    "                if token_id == tokenizer.eos_token_id or token_id == 1:  # EOS token",
    "                    break",
    "                if token_id != tokenizer.pad_token_id and token_id != 0:",
    "                    length += 1",
    "            gen_lens.append(length)",
    "        ",
    "        avg_gen_len = np.mean(gen_lens) if gen_lens else 0",
    "        result[\"gen_len\"] = avg_gen_len        ",
    "        # Convert ROUGE scores to percentages (but NOT gen_len)",
    "        final_result = {}",
    "        for k, v in result.items():",
    "            if k == \"gen_len\":",
    "                final_result[k] = round(v, 2)  # Keep gen_len as-is, just round",
    "            else:",
    "                final_result[k] = round(v * 100, 4)  # Convert ROUGE to percentage        return final_result",
    "    except Exception as e:",
    "        import traceback",
    "        traceback.print_exc()",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0, \"rougeLsum\": 0.0, \"gen_len\": 0.0}",
    "",
    "",
    "# Create data collator",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)",
    "",
    "# Get a small batch",
    "sample_batch = [tokenized_train[i] for i in range(2)]",
    "collated_batch = data_collator(sample_batch)",
    "",
    "print(f\"Batch keys: {list(collated_batch.keys())}\")",
    "print(f\"Input IDs shape: {collated_batch['input_ids'].shape}\")",
    "print(f\"Labels shape: {collated_batch['labels'].shape}\")",
    "print(f\"\\nSample input IDs (first 20): {collated_batch['input_ids'][0][:20].tolist()}\")",
    "print(f\"Sample labels (first 20): {collated_batch['labels'][0][:20].tolist()}\")",
    "",
    "# Count how many labels are NOT -100 (i.e., actual labels vs padding)",
    "labels_array = collated_batch['labels'].numpy()",
    "non_ignore_labels = np.sum(labels_array != -100)",
    "total_labels = labels_array.size",
    "ignore_ratio = (total_labels - non_ignore_labels) / total_labels",
    "",
    "print(f\"\\nLabel statistics:\")",
    "print(f\"  Total label positions: {total_labels}\")",
    "print(f\"  Non-ignore labels (not -100): {non_ignore_labels}\")",
    "print(f\"  Ignore labels (-100): {total_labels - non_ignore_labels}\")",
    "print(f\"  Ignore ratio: {ignore_ratio:.2%}\")",
    "",
    "if ignore_ratio > 0.95:",
    "    print(\"   This will cause very low or zero loss!\")",
    "",
    "# Try a forward pass to see actual loss",
    "print(\"\\n\" + \"=\" * 80)",
    "",
    "model.eval()",
    "with torch.no_grad():",
    "    # Move batch to device",
    "    batch_device = {k: v.to(device) for k, v in collated_batch.items()}",
    "    outputs = model(**batch_device)",
    "    loss = outputs.loss",
    "    loss_value = loss.item()",
    "    print(f\"Forward pass loss: {loss_value:.6f}\")",
    "    ",
    "    if np.isnan(loss_value):",
    "        print(\"   This indicates a GPU-specific numerical instability issue.\")",
    "        print(f\"   Model device: {next(model.parameters()).device}\")",
    "        print(f\"   Batch device: {batch_device['input_ids'].device}\")",
    "        print(f\"   Model dtype: {next(model.parameters()).dtype}\")",
    "    elif loss_value == 0.0:",
    "        print(\"   This confirms the model is not computing loss correctly.\")",
    "        print(\"   The issue is likely with label preparation.\")",
    "    else:",
    "        print(f\"\u2713 Forward pass loss looks good: {loss_value:.6f} - Model should learn!\")",
    "",
    "print(\"=\" * 80)",
    "",
    "",
    "print(\"=\" * 80)",
    "print(\"STEP 6: Setting up training...\")",
    "print(\"=\" * 80)",
    "",
    "# GPU-optimized training arguments with BF16 support",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 7",
    "",
    "args = Seq2SeqTrainingArguments(",
    "    output_dir=\"./flan-t5-summarizer\",",
    "    eval_strategy=\"epoch\",",
    "    learning_rate=2e-4,",
    "    per_device_train_batch_size=4,",
    "    per_device_eval_batch_size=4,",
    "    weight_decay=0.01,",
    "    save_total_limit=1,",
    "    num_train_epochs=2,",
    "    predict_with_generate=True,",
    "    generation_max_length=MAX_TARGET_LENGTH,",
    "    generation_num_beams=4,",
    "    fp16=False,  # Don't use FP16",
    "    logging_steps=100,",
    "    save_strategy=\"epoch\",",
    "    load_best_model_at_end=True,",
    "    metric_for_best_model=\"rouge1\",",
    ")",
    "",
    "print(f\"Training configuration:\")",
    "print(f\"  Epochs: {args.num_train_epochs}\")",
    "print(f\"  Batch size: {args.per_device_train_batch_size}\")",
    "print(f\"  Learning rate: {args.learning_rate}\")",
    "print(f\"  Generation max length: {args.generation_max_length}\")",
    "print(f\"  Generation num beams: {args.generation_num_beams}\")",
    "print(f\"  BF16: {args.bf16} (GPU-optimized for {'modern' if use_bf16 else 'older'} GPU)\")",
    "print(f\"  FP16: {args.fp16}\")",
    "print(f\"  Output dir: {args.output_dir}\")",
    "",
    "print(\"\\nCreating trainer...\")",
    "trainer = Seq2SeqTrainer(",
    "    model=model,",
    "    args=args,",
    "    train_dataset=tokenized_train,",
    "    eval_dataset=tokenized_val,",
    "    data_collator=data_collator,",
    "    tokenizer=tokenizer,",
    "    compute_metrics=compute_metrics,",
    ")",
    "print(f\"\u2713 Trainer created\")",
    "print(f\"  Train samples: {len(tokenized_train)}\")",
    "print(f\"  Eval samples: {len(tokenized_val)}\")",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"Starting training...\")",
    "print(\"=\" * 80)",
    "",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36373d51",
   "metadata": {},
   "source": [
    "## 7. Evaluation: Summarization Quality\n",
    "\n",
    "Compare ROUGE scores and look at qualitative examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e603b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STEP 7: Evaluating on test set...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Test samples: {len(tokenized_test)}\")\n",
    "\n",
    "try:\n",
    "    test_results = trainer.evaluate(tokenized_test)\n",
    "    print(\"\\n\u2713 Test evaluation complete!\")\n",
    "    print(\"\\nTest Results:\")\n",
    "    for key, value in test_results.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    print(\"=\" * 80)\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Test evaluation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative Comparison\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 8: Qualitative Comparison...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def generate_summary(model_obj, text, device):\n",
    "    try:\n",
    "        inputs = tokenizer(PREFIX + text, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True).to(device)\n",
    "        outputs = model_obj.generate(inputs.input_ids, max_length=MAX_TARGET_LENGTH, num_beams=4)\n",
    "        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] generate_summary failed: {e}\")\n",
    "        return f\"[ERROR: {str(e)}]\"\n",
    "\n",
    "sample_indices = [0, 5, 10, 15, 20]\n",
    "print(f\"Comparing {len(sample_indices)} examples from test set...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    try:\n",
    "        print(f\"\\nExample {i}/{len(sample_indices)} (Index {idx}):\")\n",
    "        example = test_ds[idx]\n",
    "        text = example[\"Text\"]\n",
    "        ref_summary = example[\"Summary\"]\n",
    "        \n",
    "        print(f\"  Generating base model summary...\")\n",
    "        base_summary = generate_summary(base_model, text, device)\n",
    "        \n",
    "        print(f\"  Generating fine-tuned model summary...\")\n",
    "        ft_summary = generate_summary(model, text, device)\n",
    "        \n",
    "        print(f\"\\n  Review: {text[:200]}...\")\n",
    "        print(f\"  Reference: {ref_summary}\")\n",
    "        print(f\"  Base Model: {base_summary}\")\n",
    "        print(f\"  Fine-Tuned: {ft_summary}\")\n",
    "        print(\"-\" * 80)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to process example {idx}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\u2713 Qualitative comparison complete!\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3407042",
   "metadata": {},
   "source": [
    "## 8. Forgetting Analysis (After Training)\n",
    "\n",
    "Check if the fine-tuned model has forgotten general knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36285762",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STEP 9: Evaluating Fine-Tuned Model on QA set (After Training)...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Number of QA pairs: {len(qa_pairs)}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Fine-tuned model on device: {next(model.parameters()).device}\")\n",
    "\n",
    "try:\n",
    "    ft_qa_acc, ft_qa_results = evaluate_forgetting(model, tokenizer, qa_pairs, device)\n",
    "    print(f\"\\n\u2713 Fine-tuned model evaluation complete!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FORGETTING ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Base Model QA Accuracy: {base_qa_acc:.2%} ({base_qa_acc * len(qa_pairs):.0f}/{len(qa_pairs)})\")\n",
    "    print(f\"Fine-Tuned Model QA Accuracy: {ft_qa_acc:.2%} ({ft_qa_acc * len(qa_pairs):.0f}/{len(qa_pairs)})\")\n",
    "    \n",
    "    diff = ft_qa_acc - base_qa_acc\n",
    "    print(f\"Change in Accuracy: {diff:+.2%}\")\n",
    "    \n",
    "    if diff < 0:\n",
    "        print(f\"\u26a0\ufe0f  Forgetting detected! Model lost {abs(diff):.2%} accuracy on general knowledge.\")\n",
    "    elif diff > 0:\n",
    "        print(f\"\u2713 Model improved by {diff:.2%} (unexpected but good!)\")\n",
    "    else:\n",
    "        print(f\"\u2192 No change in general knowledge performance.\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Fine-tuned model evaluation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd78ec",
   "metadata": {},
   "source": [
    "## 9. Save Model\n",
    "\n",
    "Save the fine-tuned model to be downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0503d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./finetuned_summarizer_final\")\n",
    "tokenizer.save_pretrained(\"./finetuned_summarizer_final\")\n",
    "\n",
    "print(\"Model saved to ./finetuned_summarizer_final\")\n",
    "# To download from Colab:\n",
    "# from google.colab import files\n",
    "# !zip -r model.zip ./finetuned_summarizer_final\n",
    "# files.download('model.zip')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f3bfb3",
   "metadata": {},
   "source": [
    "# Project 3: Fine-Tuning FLAN-T5 for Summarization & Measuring Forgetting\n",
    "\n",
    "**Authors:** Shaunak Kapur & Pranav Krishnan\n",
    "\n",
    "This notebook implements the Project 3 proposal: fine-tuning a small language model (`google/flan-t5-small`) on the Amazon Fine Food Reviews dataset to generate product review summaries. It also evaluates \"forgetting\" by checking the model's performance on a set of general knowledge questions before and after fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c474534",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Installing required libraries: `transformers`, `datasets`, `evaluate`, `rouge_score`, `accelerate`, `sentencepiece`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda1d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate rouge_score accelerate sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# The code below was generated by AI; see [2].\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63d2cd1",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data\n",
    "\n",
    "We use the Amazon Fine Food Reviews dataset. \n",
    "**Important:** You must upload `Reviews.csv` to the Colab runtime files (left sidebar) before running this cell.\n",
    "\n",
    "We will:\n",
    "1. Load the CSV.\n",
    "2. Drop rows with missing values.\n",
    "3. Sample the data (e.g., 20,000 rows) to keep training time reasonable.\n",
    "4. Split into Train (80%), Validation (10%), and Test (10%).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556933dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# The code below was generated by AI; see [2].\n",
    "try:\n",
    "    df = pd.read_csv(\"Reviews.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Reviews.csv not found. Please upload it to the Colab runtime.\")\n",
    "    # Create dummy data for demonstration purposes if file is missing so notebook can still 'run' structurally\n",
    "    data = {\n",
    "        \"Summary\": [\"Great product\", \"Not good\", \"Okay item\"] * 100,\n",
    "        \"Text\": [\"This is a really great product I loved it.\", \"This was terrible do not buy.\", \"It was just okay nothing special.\"] * 100\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "# Keep relevant columns and drop NaNs\n",
    "df = df[[\"Summary\", \"Text\"]].dropna()\n",
    "\n",
    "# Filter out very long reviews to save memory/time\n",
    "df = df[df[\"Text\"].str.len() <= 512]\n",
    "\n",
    "# Sample data for faster training (adjust as needed)\n",
    "SAMPLE_SIZE = 20000\n",
    "if len(df) > SAMPLE_SIZE:\n",
    "    df = df.sample(SAMPLE_SIZE, random_state=42)\n",
    "\n",
    "print(f\"Dataset size: {len(df)}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89942d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The code below was generated by AI; see [2].\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "val_ds = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "test_ds = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "print(f\"Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e28c502",
   "metadata": {},
   "source": [
    "## 3. Model and Tokenizer Setup\n",
    "\n",
    "We use `google/flan-t5-small`. We load two copies:\n",
    "1. `base_model`: Keeps original weights to measure baseline performance and forgetting.\n",
    "2. `model`: Will be fine-tuned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf2c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Model to be fine-tuned\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Base model for comparison (frozen)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "base_model.to(device)\n",
    "print(\"Models loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b6d2c",
   "metadata": {},
   "source": [
    "## 4. Tokenization\n",
    "\n",
    "We preprocess the text inputs with a prefix \"Summarize this review: \".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511ba2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_INPUT_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 32\n",
    "PREFIX = \"Summarize this review: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [PREFIX + doc for doc in examples[\"Text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples[\"Summary\"], max_length=MAX_TARGET_LENGTH, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = train_ds.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_ds.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_ds.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df861d9",
   "metadata": {},
   "source": [
    "## 5. Forgetting Analysis (Before Training)\n",
    "\n",
    "We define a small set of general knowledge questions to test the \"forgetting\" hypothesis. We check how well the base model answers them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2964199",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = [\n",
    "    (\"What is the capital of France?\", \"Paris\"),\n",
    "    (\"How many days are in a week?\", \"7\"),\n",
    "    (\"What gas do plants absorb?\", \"carbon dioxide\"),\n",
    "    (\"What is the largest planet in our solar system?\", \"Jupiter\"),\n",
    "    (\"What is H2O?\", \"water\"),\n",
    "    (\"Who wrote Romeo and Juliet?\", \"Shakespeare\"),\n",
    "    (\"What color is the sky on a clear day?\", \"blue\"),\n",
    "    (\"What is 2 + 2?\", \"4\")\n",
    "]\n",
    "\n",
    "def evaluate_forgetting(model_obj, tokenizer_obj, questions, device):\n",
    "    model_obj.eval()\n",
    "    correct = 0\n",
    "    results = []\n",
    "    \n",
    "    print(\"--- Forgetting Analysis ---\")\n",
    "    for q, ans in questions:\n",
    "        input_ids = tokenizer_obj(\"Answer the question: \" + q, return_tensors=\"pt\").input_ids.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model_obj.generate(input_ids, max_length=20)\n",
    "        \n",
    "        pred = tokenizer_obj.decode(outputs[0], skip_special_tokens=True)\n",
    "        is_correct = ans.lower() in pred.lower()\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            \n",
    "        results.append({\"Question\": q, \"Expected\": ans, \"Predicted\": pred, \"Correct\": is_correct})\n",
    "        print(f\"Q: {q} | Pred: {pred} | Expected: {ans}\")\n",
    "    \n",
    "    accuracy = correct / len(questions)\n",
    "    print(f\"Accuracy: {accuracy:.2%}\")\n",
    "    return accuracy, results\n",
    "\n",
    "print(\"Evaluating Base Model on QA set...\")\n",
    "base_qa_acc, base_qa_results = evaluate_forgetting(base_model, tokenizer, qa_pairs, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e64b2d0",
   "metadata": {},
   "source": [
    "## 6. Fine-Tuning\n",
    "\n",
    "We use `Seq2SeqTrainer` to fine-tune the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc5022",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan-t5-summarizer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=(device == \"cuda\"),\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# The code below was generated by AI; see [2].\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b48a58",
   "metadata": {},
   "source": [
    "## 7. Evaluation: Summarization Quality\n",
    "\n",
    "Compare ROUGE scores and look at qualitative examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6187b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on Test Set...\")\n",
    "test_results = trainer.evaluate(tokenized_test)\n",
    "print(test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e43bb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative Comparison\n",
    "def generate_summary(model_obj, text, device):\n",
    "    inputs = tokenizer(PREFIX + text, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True).to(device)\n",
    "    outputs = model_obj.generate(inputs.input_ids, max_length=MAX_TARGET_LENGTH, num_beams=4)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "sample_indices = [0, 5, 10, 15, 20]\n",
    "print(\"--- Qualitative Results ---\\n\")\n",
    "\n",
    "for idx in sample_indices:\n",
    "    example = test_ds[idx]\n",
    "    text = example[\"Text\"]\n",
    "    ref_summary = example[\"Summary\"]\n",
    "    \n",
    "    base_summary = generate_summary(base_model, text, device)\n",
    "    ft_summary = generate_summary(model, text, device)\n",
    "    \n",
    "    print(f\"Review: {text[:200]}...\")\n",
    "    print(f\"Reference: {ref_summary}\")\n",
    "    print(f\"Base Model: {base_summary}\")\n",
    "    print(f\"Fine-Tuned: {ft_summary}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39537a45",
   "metadata": {},
   "source": [
    "## 8. Forgetting Analysis (After Training)\n",
    "\n",
    "Check if the fine-tuned model has forgotten general knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73520c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Fine-Tuned Model on QA set...\")\n",
    "ft_qa_acc, ft_qa_results = evaluate_forgetting(model, tokenizer, qa_pairs, device)\n",
    "\n",
    "print(f\"\\nBase Model QA Accuracy: {base_qa_acc:.2%}\")\n",
    "print(f\"Fine-Tuned Model QA Accuracy: {ft_qa_acc:.2%}\")\n",
    "\n",
    "diff = ft_qa_acc - base_qa_acc\n",
    "print(f\"Change in Accuracy: {diff:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5228e4d8",
   "metadata": {},
   "source": [
    "## 9. Save Model\n",
    "\n",
    "Save the fine-tuned model to be downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1f7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./finetuned_summarizer_final\")\n",
    "tokenizer.save_pretrained(\"./finetuned_summarizer_final\")\n",
    "\n",
    "print(\"Model saved to ./finetuned_summarizer_final\")\n",
    "# To download from Colab:\n",
    "# from google.colab import files\n",
    "# !zip -r model.zip ./finetuned_summarizer_final\n",
    "# files.download('model.zip')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}